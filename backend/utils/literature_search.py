#!/usr/bin/env python3
"""
æ–‡çŒ®æœç´¢æ¨¡å— - æ”¯æŒå¤šä¸ªå­¦æœ¯æ•°æ®åº“çš„æ–‡çŒ®æ£€ç´¢
"""
import asyncio
import aiohttp
import httpx
import json
import time
import feedparser
import xml.etree.ElementTree as ET
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from urllib.parse import quote, urlencode
from loguru import logger
import re
from datetime import datetime, timedelta


@dataclass
class LiteratureSearchResult:
    """æ–‡çŒ®æœç´¢ç»“æœ"""
    title: str
    authors: List[str]
    abstract: str
    publication_date: str
    journal: str
    doi: Optional[str] = None
    url: Optional[str] = None
    keywords: List[str] = None
    citation_count: int = 0
    quality_score: float = 0.0
    source_database: str = ""


@dataclass
class SearchQuery:
    """æœç´¢æŸ¥è¯¢"""
    keywords: List[str]
    title_keywords: Optional[List[str]] = None
    author: Optional[str] = None
    journal: Optional[str] = None
    year_range: Optional[tuple] = None
    max_results: int = 50
    language: str = "en"


@dataclass
class APIUsageStats:
    """APIä½¿ç”¨ç»Ÿè®¡"""
    api_name: str
    requests_count: int = 0
    cost_estimate: float = 0.0
    last_request_time: Optional[datetime] = None
    error_count: int = 0
    daily_limit_reached: bool = False


class LiteratureSearchEngine:
    """å¢å¼ºçš„æ–‡çŒ®æœç´¢å¼•æ“ - æ”¯æŒå¤šä¸ªAPIå’Œè´¹ç”¨ç›‘æ§"""
    
    def __init__(self, config=None):
        self.config = config
        self.search_stats = {
            "total_searches": 0,
            "successful_searches": 0,
            "total_results": 0,
            "average_response_time": 0.0
        }
        self.response_times = []
        
        # APIä½¿ç”¨ç»Ÿè®¡
        self.api_usage = {
            "serpapi": APIUsageStats("SerpApi"),
            "searchapi": APIUsageStats("SearchApi"),
            "arxiv": APIUsageStats("arXiv"),
            "semantic_scholar": APIUsageStats("Semantic Scholar"),
            "pubmed": APIUsageStats("PubMed"),
            "crossref": APIUsageStats("CrossRef")
        }
        
        # APIå®šä»·ï¼ˆæ¯1000æ¬¡è¯·æ±‚çš„ä¼°è®¡è´¹ç”¨ï¼‰
        self.api_pricing = {
            "serpapi": 5.0,  # SerpApiå¤§çº¦$5/1000æ¬¡
            "searchapi": 4.0,  # SearchApiå¤§çº¦$4/1000æ¬¡
            "arxiv": 0.0,  # å…è´¹
            "semantic_scholar": 0.0,  # å…è´¹
            "pubmed": 0.0,  # å…è´¹
            "crossref": 0.0  # å…è´¹
        }
        
        # å½“æ—¥ä½¿ç”¨ç»Ÿè®¡
        self.daily_cost = 0.0
        self.last_reset_date = datetime.now().date()
    
    def _reset_daily_stats_if_needed(self):
        """å¦‚æœæ˜¯æ–°çš„ä¸€å¤©ï¼Œé‡ç½®æ¯æ—¥ç»Ÿè®¡"""
        today = datetime.now().date()
        if today != self.last_reset_date:
            self.daily_cost = 0.0
            self.last_reset_date = today
            for api_stat in self.api_usage.values():
                api_stat.daily_limit_reached = False
            logger.info("æ¯æ—¥APIä½¿ç”¨ç»Ÿè®¡å·²é‡ç½®")
    
    def _check_api_cost_limit(self, api_name: str) -> bool:
        """æ£€æŸ¥APIè´¹ç”¨æ˜¯å¦è¶…é™"""
        self._reset_daily_stats_if_needed()
        
        if not self.config or not self.config.api_cost_monitoring:
            return True
        
        estimated_cost = self.api_pricing.get(api_name, 0.0) / 1000
        
        if self.daily_cost + estimated_cost > self.config.daily_cost_limit:
            logger.warning(f"âš ï¸ APIè´¹ç”¨å³å°†è¶…è¿‡æ¯æ—¥é™åˆ¶: {self.daily_cost:.2f}/${self.config.daily_cost_limit}")
            self.api_usage[api_name].daily_limit_reached = True
            return False
        
        if (self.daily_cost + estimated_cost) / self.config.daily_cost_limit > self.config.cost_alert_threshold:
            logger.warning(f"âš ï¸ APIè´¹ç”¨å·²è¾¾è­¦æˆ’çº¿: {(self.daily_cost / self.config.daily_cost_limit * 100):.1f}%")
        
        return True
    
    def _record_api_usage(self, api_name: str, success: bool = True):
        """è®°å½•APIä½¿ç”¨æƒ…å†µ"""
        if api_name in self.api_usage:
            self.api_usage[api_name].requests_count += 1
            self.api_usage[api_name].last_request_time = datetime.now()
            
            if success:
                cost = self.api_pricing.get(api_name, 0.0) / 1000
                self.api_usage[api_name].cost_estimate += cost
                self.daily_cost += cost
                
                if self.config and self.config.api_usage_log_enabled:
                    logger.info(f"ğŸ“Š APIä½¿ç”¨: {api_name}, ä»Šæ—¥è´¹ç”¨: ${self.daily_cost:.4f}")
            else:
                self.api_usage[api_name].error_count += 1
    
    def _select_optimal_api(self, query: SearchQuery) -> str:
        """æ™ºèƒ½é€‰æ‹©æœ€ä¼˜API"""
        if not self.config:
            return "arxiv"  # é»˜è®¤ä½¿ç”¨å…è´¹API
        
        strategy = getattr(self.config, 'search_strategy', 'intelligent')
        
        if strategy == "free_first":
            # ä¼˜å…ˆä½¿ç”¨å…è´¹API
            for api in ["arxiv", "semantic_scholar", "pubmed", "crossref"]:
                if not self.api_usage[api].daily_limit_reached:
                    return api
        
        elif strategy == "quality_first":
            # ä¼˜å…ˆä½¿ç”¨é«˜è´¨é‡ä»˜è´¹API
            if self._check_api_cost_limit("searchapi") and not self.api_usage["searchapi"].daily_limit_reached:
                return "searchapi"
            elif self._check_api_cost_limit("serpapi") and not self.api_usage["serpapi"].daily_limit_reached:
                return "serpapi"
        
        elif strategy == "intelligent":
            # æ™ºèƒ½é€‰æ‹©ï¼šæ ¹æ®æŸ¥è¯¢ç±»å‹å’ŒAPIå¯ç”¨æ€§
            academic_keywords = ["machine learning", "deep learning", "AI", "neural network", "algorithm"]
            is_academic = any(kw.lower() in " ".join(query.keywords).lower() for kw in academic_keywords)
            
            if is_academic:
                # å­¦æœ¯æŸ¥è¯¢ä¼˜å…ˆä½¿ç”¨å…è´¹çš„å­¦æœ¯æ•°æ®åº“
                for api in ["arxiv", "semantic_scholar"]:
                    if not self.api_usage[api].daily_limit_reached:
                        return api
            
            # å…¶ä»–æŸ¥è¯¢å¯ä»¥ä½¿ç”¨ä»˜è´¹API
            if self._check_api_cost_limit("searchapi"):
                return "searchapi"
        
        # å›é€€åˆ°å…è´¹API
        return "arxiv"

    async def search_serpapi_google_scholar(self, query: SearchQuery) -> List[LiteratureSearchResult]:
        """ä½¿ç”¨SerpApiæœç´¢Google Scholar"""
        if not self.config or not getattr(self.config, 'serpapi_key', None):
            logger.warning("SerpApiå¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡æœç´¢")
            return []
        
        if not self._check_api_cost_limit("serpapi"):
            logger.warning("SerpApiå·²è¾¾è´¹ç”¨é™åˆ¶ï¼Œè·³è¿‡æœç´¢")
            return []
        
        logger.info(f"âœ¨ ä½¿ç”¨SerpApiæœç´¢Google Scholar: {query.keywords}")
        start_time = time.time()
        
        try:
            search_terms = " ".join(query.keywords)
            
            params = {
                'engine': 'google_scholar',
                'q': search_terms,
                'api_key': self.config.serpapi_key,
                'num': min(query.max_results, 20),  # SerpApié™åˆ¶
                'hl': 'en'
            }
            
            if query.year_range:
                start_year, end_year = query.year_range
                params['as_ylo'] = start_year
                params['as_yhi'] = end_year
            
            url = f"https://serpapi.com/search?{urlencode(params)}"
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url)
                
                if response.status_code == 200:
                    data = response.json()
                    results = self._parse_serpapi_response(data)
                    
                    self._record_api_usage("serpapi", True)
                    response_time = time.time() - start_time
                    self._update_stats(len(results), response_time, True)
                    
                    logger.info(f"âœ… SerpApiæœç´¢æˆåŠŸ: {len(results)}ç¯‡æ–‡çŒ®, è€—æ—¶{response_time:.2f}s")
                    return results
                else:
                    logger.error(f"âŒ SerpApiæœç´¢å¤±è´¥: HTTP {response.status_code}")
                    if response.status_code == 401:
                        logger.error("SerpApi APIå¯†é’¥æ— æ•ˆæˆ–ä½™é¢ä¸è¶³")
                    self._record_api_usage("serpapi", False)
                    return []
        
        except Exception as e:
            logger.error(f"âŒ SerpApiæœç´¢å¼‚å¸¸: {e}")
            self._record_api_usage("serpapi", False)
            return []

    async def search_searchapi_google_scholar(self, query: SearchQuery) -> List[LiteratureSearchResult]:
        """ä½¿ç”¨SearchApiæœç´¢Google Scholar"""
        if not self.config or not getattr(self.config, 'searchapi_key', None):
            logger.warning("SearchApiå¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡æœç´¢")
            return []
        
        if not self._check_api_cost_limit("searchapi"):
            logger.warning("SearchApiå·²è¾¾è´¹ç”¨é™åˆ¶ï¼Œè·³è¿‡æœç´¢")
            return []
        
        logger.info(f"âœ¨ ä½¿ç”¨SearchApiæœç´¢Google Scholar: {query.keywords}")
        start_time = time.time()
        
        try:
            search_terms = " ".join(query.keywords)
            
            params = {
                'engine': 'google_scholar',
                'q': search_terms,
                'api_key': self.config.searchapi_key,
                'num': min(query.max_results, 20),  # SearchApié™åˆ¶
                'hl': 'en'
            }
            
            if query.year_range:
                start_year, end_year = query.year_range
                params['as_ylo'] = start_year
                params['as_yhi'] = end_year
            
            url = f"https://www.searchapi.io/api/v1/search?{urlencode(params)}"
            
            headers = {
                'Authorization': f'Bearer {self.config.searchapi_key}',
                'Content-Type': 'application/json'
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    results = self._parse_searchapi_response(data)
                    
                    self._record_api_usage("searchapi", True)
                    response_time = time.time() - start_time
                    self._update_stats(len(results), response_time, True)
                    
                    logger.info(f"âœ… SearchApiæœç´¢æˆåŠŸ: {len(results)}ç¯‡æ–‡çŒ®, è€—æ—¶{response_time:.2f}s")
                    return results
                else:
                    logger.error(f"âŒ SearchApiæœç´¢å¤±è´¥: HTTP {response.status_code}")
                    if response.status_code == 401:
                        logger.error("SearchApi APIå¯†é’¥æ— æ•ˆæˆ–ä½™é¢ä¸è¶³")
                    elif response.status_code == 429:
                        logger.error("SearchApiè¯·æ±‚é¢‘ç‡è¶…é™")
                    self._record_api_usage("searchapi", False)
                    return []
        
        except Exception as e:
            logger.error(f"âŒ SearchApiæœç´¢å¼‚å¸¸: {e}")
            self._record_api_usage("searchapi", False)
            return []

    def _parse_serpapi_response(self, data: dict) -> List[LiteratureSearchResult]:
        """è§£æSerpApiå“åº”"""
        results = []
        
        organic_results = data.get('organic_results', [])
        for item in organic_results:
            try:
                result = LiteratureSearchResult(
                    title=item.get('title', ''),
                    authors=self._extract_authors_from_serpapi(item),
                    abstract=item.get('snippet', ''),
                    publication_date=item.get('publication_info', {}).get('summary', ''),
                    journal=self._extract_journal_from_serpapi(item),
                    url=item.get('link', ''),
                    citation_count=self._extract_citations_from_serpapi(item),
                    source_database="Google Scholar (SerpApi)"
                )
                results.append(result)
            except Exception as e:
                logger.warning(f"è§£æSerpApiç»“æœæ—¶å‡ºé”™: {e}")
                continue
        
        return results

    def _parse_searchapi_response(self, data: dict) -> List[LiteratureSearchResult]:
        """è§£æSearchApiå“åº”"""
        results = []
        
        organic_results = data.get('organic_results', [])
        for item in organic_results:
            try:
                result = LiteratureSearchResult(
                    title=item.get('title', ''),
                    authors=self._extract_authors_from_searchapi(item),
                    abstract=item.get('snippet', ''),
                    publication_date=item.get('publication_info', {}).get('summary', ''),
                    journal=self._extract_journal_from_searchapi(item),
                    url=item.get('link', ''),
                    citation_count=self._extract_citations_from_searchapi(item),
                    source_database="Google Scholar (SearchApi)"
                )
                results.append(result)
            except Exception as e:
                logger.warning(f"è§£æSearchApiç»“æœæ—¶å‡ºé”™: {e}")
                continue
        
        return results

    def _extract_authors_from_serpapi(self, item: dict) -> List[str]:
        """ä»SerpApiç»“æœä¸­æå–ä½œè€…"""
        publication_info = item.get('publication_info', {})
        authors_str = publication_info.get('authors', '')
        if authors_str:
            return [author.strip() for author in authors_str.split(',')]
        return []

    def _extract_authors_from_searchapi(self, item: dict) -> List[str]:
        """ä»SearchApiç»“æœä¸­æå–ä½œè€…"""
        publication_info = item.get('publication_info', {})
        authors_str = publication_info.get('authors', '')
        if authors_str:
            return [author.strip() for author in authors_str.split(',')]
        return []

    def _extract_journal_from_serpapi(self, item: dict) -> str:
        """ä»SerpApiç»“æœä¸­æå–æœŸåˆŠ"""
        publication_info = item.get('publication_info', {})
        return publication_info.get('summary', '').split(' - ')[0] if publication_info.get('summary') else ''

    def _extract_journal_from_searchapi(self, item: dict) -> str:
        """ä»SearchApiç»“æœä¸­æå–æœŸåˆŠ"""
        publication_info = item.get('publication_info', {})
        return publication_info.get('summary', '').split(' - ')[0] if publication_info.get('summary') else ''

    def _extract_citations_from_serpapi(self, item: dict) -> int:
        """ä»SerpApiç»“æœä¸­æå–å¼•ç”¨æ•°"""
        inline_links = item.get('inline_links', {})
        cited_by = inline_links.get('cited_by', {})
        total = cited_by.get('total', 0)
        return int(total) if isinstance(total, (str, int)) and str(total).isdigit() else 0

    def _extract_citations_from_searchapi(self, item: dict) -> int:
        """ä»SearchApiç»“æœä¸­æå–å¼•ç”¨æ•°"""
        inline_links = item.get('inline_links', {})
        cited_by = inline_links.get('cited_by', {})
        total = cited_by.get('total', 0)
        return int(total) if isinstance(total, (str, int)) and str(total).isdigit() else 0

    async def search_multiple_databases(self, query: SearchQuery) -> Dict[str, List[LiteratureSearchResult]]:
        """æ™ºèƒ½æœç´¢å¤šä¸ªæ•°æ®åº“"""
        logger.info(f"ğŸš€ å¼€å§‹æ™ºèƒ½æ–‡çŒ®æœç´¢: {query.keywords}")
        start_time = time.time()
        
        # é€‰æ‹©æœ€ä¼˜APIç­–ç•¥
        primary_api = self._select_optimal_api(query)
        logger.info(f"ğŸ“‹ é€‰æ‹©çš„ä¸»è¦æœç´¢API: {primary_api}")
        
        results = {}
        
        try:
            # ä¸»è¦æœç´¢
            if primary_api == "searchapi":
                results["searchapi"] = await self.search_searchapi_google_scholar(query)
            elif primary_api == "serpapi":
                results["serpapi"] = await self.search_serpapi_google_scholar(query)
            elif primary_api == "arxiv":
                results["arxiv"] = await self.search_arxiv(query)
            elif primary_api == "semantic_scholar":
                results["semantic_scholar"] = await self.search_semantic_scholar(query)
            
            # å¦‚æœä¸»è¦æœç´¢ç»“æœä¸è¶³ï¼Œå°è¯•è¡¥å……æœç´¢
            total_results = sum(len(papers) for papers in results.values())
            if total_results < query.max_results // 2:
                logger.info("ğŸ“ˆ ä¸»è¦æœç´¢ç»“æœä¸è¶³ï¼Œå¯åŠ¨è¡¥å……æœç´¢")
                
                # è¡¥å……å…è´¹APIæœç´¢
                if primary_api != "arxiv":
                    results["arxiv"] = await self.search_arxiv(query)
                
                if primary_api != "semantic_scholar":
                    # é€‚å½“é™ä½ç»“æœæ•°é‡ä»¥é¿å…è¿‡å¤šè¯·æ±‚
                    backup_query = SearchQuery(
                        keywords=query.keywords,
                        max_results=min(query.max_results // 2, 20),
                        year_range=query.year_range
                    )
                    backup_results = await self.search_semantic_scholar(backup_query)
                    if backup_results:
                        results["semantic_scholar"] = backup_results
            
            # è®°å½•æœç´¢ç»Ÿè®¡
            total_time = time.time() - start_time
            total_papers = sum(len(papers) for papers in results.values())
            
            logger.info(f"ğŸ¯ æ™ºèƒ½æ–‡çŒ®æœç´¢å®Œæˆ: å…±æ‰¾åˆ°{total_papers}ç¯‡æ–‡çŒ®, è€—æ—¶{total_time:.2f}s")
            
            return results
        
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½æœç´¢å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºç¡€æœç´¢
            if self.config and getattr(self.config, 'fallback_to_free', True):
                logger.info("ğŸ”„ å›é€€åˆ°å…è´¹APIæœç´¢")
                try:
                    results["arxiv"] = await self.search_arxiv(query)
                    return results
                except Exception as fallback_error:
                    logger.error(f"âŒ å›é€€æœç´¢ä¹Ÿå¤±è´¥äº†: {fallback_error}")
            
            return {}

    def get_api_usage_report(self) -> dict:
        """è·å–APIä½¿ç”¨æŠ¥å‘Š"""
        self._reset_daily_stats_if_needed()
        
        report = {
            "daily_cost": self.daily_cost,
            "daily_limit": getattr(self.config, 'daily_cost_limit', 10.0) if self.config else 10.0,
            "cost_percentage": (self.daily_cost / getattr(self.config, 'daily_cost_limit', 10.0) * 100) if self.config else 0,
            "apis": {}
        }
        
        for api_name, stats in self.api_usage.items():
            report["apis"][api_name] = {
                "requests": stats.requests_count,
                "cost": stats.cost_estimate,
                "errors": stats.error_count,
                "last_used": stats.last_request_time.isoformat() if stats.last_request_time else None,
                "limit_reached": stats.daily_limit_reached
            }
        
        return report

    def _update_stats(self, result_count: int, response_time: float, success: bool):
        """æ›´æ–°æœç´¢ç»Ÿè®¡"""
        self.search_stats["total_searches"] += 1
        if success:
            self.search_stats["successful_searches"] += 1
            self.search_stats["total_results"] += result_count
        
        self.response_times.append(response_time)
        if len(self.response_times) > 100:  # ä¿æŒæœ€è¿‘100æ¬¡çš„è®°å½•
            self.response_times.pop(0)
        
        self.search_stats["average_response_time"] = sum(self.response_times) / len(self.response_times)

    def get_search_stats(self) -> dict:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.search_stats,
            "api_usage": self.get_api_usage_report()
        }

    async def search_arxiv(self, query: SearchQuery) -> List[LiteratureSearchResult]:
        """æœç´¢arXivæ•°æ®åº“ - çœŸå®APIè°ƒç”¨"""
        logger.info(f"âœ¨ æœç´¢arXiv: {query.keywords}")
        start_time = time.time()
        
        try:
            # æ„å»ºarXivæŸ¥è¯¢è¯­å¥
            search_terms = " AND ".join([f'all:"{kw}"' for kw in query.keywords])
            
            # æ·»åŠ æ—¶é—´èŒƒå›´è¿‡æ»¤
            if query.year_range:
                start_year, end_year = query.year_range
                search_terms += f" AND submittedDate:[{start_year}0101 TO {end_year}1231]"
            
            # æ„å»ºå®Œæ•´URL
            params = {
                'search_query': search_terms,
                'start': 0,
                'max_results': min(query.max_results, 100),  # arXivé™åˆ¶
                'sortBy': 'relevance',
                'sortOrder': 'descending'
            }
            
            url = f"https://export.arxiv.org/api/query?{urlencode(params)}"
            logger.info(f"ğŸ” arXivæŸ¥è¯¢URL: {url[:100]}...")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url)
                
                if response.status_code == 200:
                    content = response.text
                    results = self._parse_arxiv_response(content)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    response_time = time.time() - start_time
                    self._update_stats(len(results), response_time, True)
                    
                    logger.info(f"âœ… arXivæœç´¢æˆåŠŸ: {len(results)}ç¯‡æ–‡çŒ®, è€—æ—¶{response_time:.2f}s")
                    return results
                else:
                    logger.error(f"âŒ arXivæœç´¢å¤±è´¥: HTTP {response.status_code}")
                    return []
                        
        except Exception as e:
            logger.error(f"âŒ arXivæœç´¢å¼‚å¸¸: {e}")
            self._update_stats(0, time.time() - start_time, False)
            return []
    
    async def search_pubmed(self, query: SearchQuery) -> List[LiteratureSearchResult]:
        """æœç´¢PubMedæ•°æ®åº“ï¼ˆä½¿ç”¨NCBI E-utilities APIï¼‰"""
        logger.info(f"æœç´¢PubMed: {query.keywords}")
        start_time = time.time()
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæœç´¢è·å–PMIDs
            search_terms = " AND ".join(f'"{keyword}"' for keyword in query.keywords)
            search_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={quote(search_terms)}&retmode=json&retmax={query.max_results}"
            
            async with aiohttp.ClientSession() as session:
                # æœç´¢é˜¶æ®µ
                async with session.get(search_url) as response:
                    if response.status != 200:
                        logger.error(f"PubMedæœç´¢å¤±è´¥: HTTP {response.status}")
                        return []
                    
                    search_data = await response.json()
                    pmids = search_data.get("esearchresult", {}).get("idlist", [])
                    
                    if not pmids:
                        logger.info("PubMedæœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®")
                        return []
                
                # è·å–è¯¦ç»†ä¿¡æ¯
                if pmids:
                    pmid_list = ",".join(pmids[:20])  # é™åˆ¶ä¸€æ¬¡è·å–20ä¸ª
                    fetch_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pmid_list}&retmode=xml"
                    
                    async with session.get(fetch_url) as response:
                        if response.status == 200:
                            xml_content = await response.text()
                            results = self._parse_pubmed_response(xml_content)
                            
                            # æ›´æ–°ç»Ÿè®¡
                            response_time = time.time() - start_time
                            self._update_stats(len(results), response_time, True)
                            
                            return results
                        else:
                            logger.error(f"PubMedè¯¦æƒ…è·å–å¤±è´¥: HTTP {response.status}")
                            return []
                            
        except Exception as e:
            logger.error(f"PubMedæœç´¢å¼‚å¸¸: {e}")
            self._update_stats(0, time.time() - start_time, False)
            return []
    
    async def search_semantic_scholar(self, query: SearchQuery) -> List[LiteratureSearchResult]:
        """æœç´¢Semantic Scholaræ•°æ®åº“ - çœŸå®APIè°ƒç”¨"""
        logger.info(f"âœ¨ æœç´¢Semantic Scholar: {query.keywords}")
        start_time = time.time()
        
        try:
            # æ„å»ºæœç´¢è¯­å¥
            search_terms = " ".join(query.keywords)
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                'query': search_terms,
                'limit': min(query.max_results, 100),  # Semantic Scholaré™åˆ¶
                'fields': 'title,authors,abstract,year,journal,citationCount,url,doi,fieldsOfStudy,venue,publicationDate'
            }
            
            # æ·»åŠ æ—¶é—´è¿‡æ»¤
            if query.year_range:
                start_year, end_year = query.year_range
                params['year'] = f"{start_year}-{end_year}"
            
            url = f"https://api.semanticscholar.org/graph/v1/paper/search?{urlencode(params)}"
            logger.info(f"ğŸ” Semantic ScholaræŸ¥è¯¢: {search_terms}")
            
            headers = {
                'User-Agent': 'Research-Multi-Agent-System/1.0',
                'Accept': 'application/json'
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    results = self._parse_semantic_scholar_response(data)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    response_time = time.time() - start_time
                    self._update_stats(len(results), response_time, True)
                    
                    logger.info(f"âœ… Semantic Scholaræœç´¢æˆåŠŸ: {len(results)}ç¯‡æ–‡çŒ®, è€—æ—¶{response_time:.2f}s")
                    return results
                elif response.status_code == 429:
                    logger.warning("âš ï¸ Semantic Scholar APIé™æµï¼Œç¨åé‡è¯•")
                    await asyncio.sleep(2)
                    return await self.search_semantic_scholar(query)  # é‡è¯•ä¸€æ¬¡
                else:
                    logger.error(f"âŒ Semantic Scholaræœç´¢å¤±è´¥: HTTP {response.status_code}")
                    return []
                        
        except Exception as e:
            logger.error(f"âŒ Semantic Scholaræœç´¢å¼‚å¸¸: {e}")
            self._update_stats(0, time.time() - start_time, False)
            return []
    
    async def search_crossref(self, query: SearchQuery) -> List[LiteratureSearchResult]:
        """æœç´¢CrossRefæ•°æ®åº“ - çœŸå®APIè°ƒç”¨"""
        logger.info(f"âœ¨ æœç´¢CrossRef: {query.keywords}")
        start_time = time.time()
        
        try:
            # æ„å»ºæœç´¢è¯­å¥
            search_terms = " ".join(query.keywords)
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                'query': search_terms,
                'rows': min(query.max_results, 50),  # CrossRefé™åˆ¶
                'sort': 'relevance',
                'order': 'desc'
            }
            
            # æ·»åŠ æ—¶é—´è¿‡æ»¤
            if query.year_range:
                start_year, end_year = query.year_range
                params['filter'] = f'from-pub-date:{start_year},until-pub-date:{end_year}'
            
            url = f"https://api.crossref.org/works?{urlencode(params)}"
            logger.info(f"ğŸ” CrossRefæŸ¥è¯¢: {search_terms}")
            
            headers = {
                'User-Agent': 'Research-Multi-Agent-System/1.0 (mailto:research@example.com)',
                'Accept': 'application/json'
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    results = self._parse_crossref_response(data)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    response_time = time.time() - start_time
                    self._update_stats(len(results), response_time, True)
                    
                    logger.info(f"âœ… CrossRefæœç´¢æˆåŠŸ: {len(results)}ç¯‡æ–‡çŒ®, è€—æ—¶{response_time:.2f}s")
                    return results
                else:
                    logger.error(f"âŒ CrossRefæœç´¢å¤±è´¥: HTTP {response.status_code}")
                    return []
                        
        except Exception as e:
            logger.error(f"âŒ CrossRefæœç´¢å¼‚å¸¸: {e}")
            self._update_stats(0, time.time() - start_time, False)
            return []
    
    def _parse_crossref_response(self, data: Dict[str, Any]) -> List[LiteratureSearchResult]:
        """è§£æCrossRefå“åº”"""
        results = []
        try:
            items = data.get("message", {}).get("items", [])
            
            for item in items:
                # æå–ä½œè€…
                authors = []
                for author in item.get("author", []):
                    given = author.get("given", "")
                    family = author.get("family", "")
                    if given and family:
                        authors.append(f"{given} {family}")
                    elif family:
                        authors.append(family)
                
                # æå–æ—¥æœŸ
                pub_date = ""
                if "published-print" in item:
                    date_parts = item["published-print"].get("date-parts", [[]])
                    if date_parts and date_parts[0]:
                        pub_date = str(date_parts[0][0])  # è·å–å¹´ä»½
                elif "published-online" in item:
                    date_parts = item["published-online"].get("date-parts", [[]])
                    if date_parts and date_parts[0]:
                        pub_date = str(date_parts[0][0])
                
                # æå–æœŸåˆŠ
                journal = item.get("container-title", [""])[0] if item.get("container-title") else ""
                
                # æå–DOI
                doi = item.get("DOI", "")
                
                # æå–æ‘˜è¦ï¼ˆCrossRefé€šå¸¸æ²¡æœ‰æ‘˜è¦ï¼‰
                abstract = item.get("abstract", "")
                
                # æ„å»ºURL
                url = f"https://doi.org/{doi}" if doi else ""
                
                result = LiteratureSearchResult(
                    title=item.get("title", [""])[0] if item.get("title") else "",
                    authors=authors,
                    abstract=abstract,
                    publication_date=pub_date,
                    journal=journal,
                    doi=doi,
                    url=url,
                    keywords=[],
                    citation_count=item.get("is-referenced-by-count", 0),
                    source_database="crossref"
                )
                results.append(result)
                
        except Exception as e:
            logger.error(f"è§£æCrossRefå“åº”å¤±è´¥: {e}")
        
        return results
    
    def _parse_arxiv_response(self, xml_content: str) -> List[LiteratureSearchResult]:
        """è§£æarXivå“åº”"""
        results = []
        try:
            import xml.etree.ElementTree as ET
            root = ET.fromstring(xml_content)
            
            # arXivå‘½åç©ºé—´
            ns = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            for entry in root.findall('atom:entry', ns):
                title = entry.find('atom:title', ns)
                summary = entry.find('atom:summary', ns)
                published = entry.find('atom:published', ns)
                
                # ä½œè€…
                authors = []
                for author in entry.findall('atom:author', ns):
                    name = author.find('atom:name', ns)
                    if name is not None:
                        authors.append(name.text)
                
                # URL
                url = entry.find('atom:id', ns)
                
                if title is not None and summary is not None:
                    result = LiteratureSearchResult(
                        title=title.text.strip(),
                        authors=authors,
                        abstract=summary.text.strip(),
                        publication_date=published.text if published is not None else "",
                        journal="arXiv",
                        url=url.text if url is not None else "",
                        keywords=[],
                        source_database="arxiv"
                    )
                    results.append(result)
                    
        except Exception as e:
            logger.error(f"è§£æarXivå“åº”å¤±è´¥: {e}")
        
        return results
    
    def _parse_pubmed_response(self, xml_content: str) -> List[LiteratureSearchResult]:
        """è§£æPubMedå“åº”"""
        results = []
        try:
            import xml.etree.ElementTree as ET
            root = ET.fromstring(xml_content)
            
            for article in root.findall('.//PubmedArticle'):
                # æ ‡é¢˜
                title_elem = article.find('.//ArticleTitle')
                title = title_elem.text if title_elem is not None else ""
                
                # æ‘˜è¦
                abstract_elem = article.find('.//AbstractText')
                abstract = abstract_elem.text if abstract_elem is not None else ""
                
                # ä½œè€…
                authors = []
                for author in article.findall('.//Author'):
                    lastname = author.find('LastName')
                    forename = author.find('ForeName')
                    if lastname is not None and forename is not None:
                        authors.append(f"{forename.text} {lastname.text}")
                
                # æœŸåˆŠ
                journal_elem = article.find('.//Journal/Title')
                journal = journal_elem.text if journal_elem is not None else ""
                
                # å‘è¡¨æ—¥æœŸ
                year_elem = article.find('.//PubDate/Year')
                year = year_elem.text if year_elem is not None else ""
                
                # DOI
                doi_elem = article.find('.//ArticleId[@IdType="doi"]')
                doi = doi_elem.text if doi_elem is not None else None
                
                if title and abstract:
                    result = LiteratureSearchResult(
                        title=title,
                        authors=authors,
                        abstract=abstract,
                        publication_date=year,
                        journal=journal,
                        doi=doi,
                        keywords=[],
                        source_database="pubmed"
                    )
                    results.append(result)
                    
        except Exception as e:
            logger.error(f"è§£æPubMedå“åº”å¤±è´¥: {e}")
        
        return results
    
    def _parse_semantic_scholar_response(self, data: Dict[str, Any]) -> List[LiteratureSearchResult]:
        """è§£æSemantic Scholarå“åº”"""
        results = []
        try:
            for paper in data.get("data", []):
                authors = [author.get("name", "") for author in paper.get("authors", [])]
                
                result = LiteratureSearchResult(
                    title=paper.get("title", ""),
                    authors=authors,
                    abstract=paper.get("abstract", ""),
                    publication_date=str(paper.get("year", "")),
                    journal=paper.get("journal", {}).get("name", "") if paper.get("journal") else "",
                    url=paper.get("url", ""),
                    citation_count=paper.get("citationCount", 0),
                    keywords=[],
                    source_database="semantic_scholar"
                )
                results.append(result)
                
        except Exception as e:
            logger.error(f"è§£æSemantic Scholarå“åº”å¤±è´¥: {e}")
        
        return results


class LiteratureQualityEvaluator:
    """æ–‡çŒ®è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.quality_weights = {
            "citation_count": 0.3,
            "journal_impact": 0.25,
            "recency": 0.2,
            "relevance": 0.15,
            "completeness": 0.1
        }
    
    def evaluate_literature(self, literature: LiteratureSearchResult, query: SearchQuery) -> float:
        """è¯„ä¼°æ–‡çŒ®è´¨é‡"""
        scores = {}
        
        # å¼•ç”¨æ•°é‡è¯„åˆ†
        scores["citation_count"] = min(literature.citation_count / 100, 1.0)
        
        # æœŸåˆŠå½±å“åŠ›è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰
        scores["journal_impact"] = self._evaluate_journal_impact(literature.journal)
        
        # æ—¶æ•ˆæ€§è¯„åˆ†
        scores["recency"] = self._evaluate_recency(literature.publication_date)
        
        # ç›¸å…³æ€§è¯„åˆ†
        scores["relevance"] = self._evaluate_relevance(literature, query)
        
        # å®Œæ•´æ€§è¯„åˆ†
        scores["completeness"] = self._evaluate_completeness(literature)
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = sum(
            scores[factor] * weight 
            for factor, weight in self.quality_weights.items()
        )
        
        return min(total_score * 10, 10.0)  # è½¬æ¢ä¸º10åˆ†åˆ¶
    
    def _evaluate_journal_impact(self, journal: str) -> float:
        """è¯„ä¼°æœŸåˆŠå½±å“åŠ›"""
        # ç®€åŒ–çš„æœŸåˆŠå½±å“åŠ›è¯„ä¼°
        high_impact_journals = [
            "nature", "science", "cell", "pnas", "jama", "nejm", 
            "lancet", "ieee", "acm", "arxiv"
        ]
        
        journal_lower = journal.lower()
        for high_journal in high_impact_journals:
            if high_journal in journal_lower:
                return 1.0
        
        return 0.6  # é»˜è®¤ä¸­ç­‰å½±å“åŠ›
    
    def _evaluate_recency(self, publication_date: str) -> float:
        """è¯„ä¼°æ—¶æ•ˆæ€§"""
        try:
            if not publication_date:
                return 0.3
                
            year = int(publication_date[:4])
            current_year = 2024
            years_ago = current_year - year
            
            if years_ago <= 1:
                return 1.0
            elif years_ago <= 3:
                return 0.8
            elif years_ago <= 5:
                return 0.6
            elif years_ago <= 10:
                return 0.4
            else:
                return 0.2
                
        except (ValueError, IndexError):
            return 0.3
    
    def _evaluate_relevance(self, literature: LiteratureSearchResult, query: SearchQuery) -> float:
        """è¯„ä¼°ç›¸å…³æ€§"""
        # ç®€åŒ–çš„ç›¸å…³æ€§è¯„ä¼°
        text_to_check = f"{literature.title} {literature.abstract}".lower()
        keyword_matches = 0
        
        for keyword in query.keywords:
            if keyword.lower() in text_to_check:
                keyword_matches += 1
        
        if query.keywords:
            return keyword_matches / len(query.keywords)
        return 0.5
    
    def _evaluate_completeness(self, literature: LiteratureSearchResult) -> float:
        """è¯„ä¼°å®Œæ•´æ€§"""
        completeness_score = 0.0
        
        if literature.title:
            completeness_score += 0.3
        if literature.abstract:
            completeness_score += 0.4
        if literature.authors:
            completeness_score += 0.15
        if literature.journal:
            completeness_score += 0.1
        if literature.publication_date:
            completeness_score += 0.05
        
        return completeness_score


# å¯¼å‡ºä¸»è¦ç±»
__all__ = [
    "LiteratureSearchEngine",
    "LiteratureQualityEvaluator", 
    "LiteratureSearchResult",
    "SearchQuery"
] 