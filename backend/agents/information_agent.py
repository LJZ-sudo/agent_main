"""
ä¿¡æ¯è·å–Agent - é›†æˆå…³é”®è¯é©±åŠ¨å’Œä¸»é¢˜å»ºæ¨¡ä¸¤ç§æ–‡çŒ®è°ƒç ”æ–¹æ³•
åŸºäº"æ–‡çŒ®è°ƒç ”åˆ†æAgentæŠ€æœ¯æ–¹æ¡ˆ V1.0.pdf"å®ç°çš„å¢å¼ºç‰ˆæ–‡çŒ®è°ƒç ”Agent
"""

import asyncio
import json
import uuid
import re
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import numpy as np

from backend.core.base_agent import BaseAgent
from backend.core.blackboard import Blackboard, BlackboardEvent, EventType, ReasoningStep
from backend.utils.literature_search import LiteratureSearchEngine, LiteratureSearchResult
from loguru import logger


@dataclass
class LiteratureDocument:
    """æ–‡çŒ®æ–‡æ¡£æ•°æ®ç»“æ„"""
    doc_id: str
    title: str
    authors: List[str]
    journal: str
    year: int
    abstract: str
    keywords: List[str]
    citation_count: int
    journal_impact_factor: float
    relevance_score: float
    quality_score: float
    source_database: str
    doi: str = ""
    full_text: str = ""


@dataclass 
class ResearchTopic:
    """ç ”ç©¶ä¸»é¢˜æ•°æ®ç»“æ„"""
    topic_id: str
    topic_name: str
    keywords: List[str]
    description: str
    coherence_score: float
    document_count: int
    representative_docs: List[str]
    trend_indicator: str  # "emerging", "stable", "declining"


@dataclass
class KnowledgeGraph:
    """çŸ¥è¯†å›¾è°±æ•°æ®ç»“æ„"""
    graph_id: str
    nodes: List[Dict[str, Any]]  # æ¦‚å¿µèŠ‚ç‚¹
    edges: List[Dict[str, Any]]  # å…³ç³»è¾¹
    central_concepts: List[str]
    connection_strength: Dict[str, float]


class InformationAgent(BaseAgent):
    """
    å¢å¼ºç‰ˆä¿¡æ¯è·å–Agent - æ™ºèƒ½æ–‡çŒ®è°ƒç ”ä¸“å®¶
    
    å®ç°ä¸‰ç§è°ƒç ”æ–¹æ³•ï¼š
    1. å…³é”®è¯é©±åŠ¨æ–‡çŒ®æ£€ç´¢æ–¹æ³•
    2. ä¸»é¢˜å»ºæ¨¡æ™ºèƒ½å‘ç°æ–¹æ³•  
    3. æ··åˆæ¨¡å¼è°ƒç ”æ–¹æ³•
    ç¬¦åˆdocsè¦æ±‚çš„å®Œæ•´RAGåŠŸèƒ½
    """

    def __init__(self, blackboard: Blackboard, llm_client=None):
        super().__init__("information_agent", blackboard)
        
        # è°ƒç ”æ–¹æ³•é…ç½®
        self.research_methods = {
            "keyword_driven": "å…³é”®è¯é©±åŠ¨æ–‡çŒ®æ£€ç´¢",
            "topic_modeling": "ä¸»é¢˜å»ºæ¨¡æ™ºèƒ½å‘ç°",
            "hybrid": "æ··åˆæ¨¡å¼è°ƒç ”"
        }
        
        # æ•°æ®åº“é…ç½®
        self.search_databases = ["IEEE", "ACM", "SpringerLink", "PubMed", "arXiv", "CNKI", "Web of Science"]
        
        # è´¨é‡è¯„ä¼°é…ç½®
        self.quality_weights = {
            "journal_impact": 0.25,    # æœŸåˆŠå½±å“å› å­
            "citation_count": 0.20,    # å¼•ç”¨æ¬¡æ•°  
            "author_authority": 0.20,  # ä½œè€…æƒå¨æ€§
            "methodology": 0.20,       # ç ”ç©¶æ–¹æ³•è´¨é‡
            "relevance": 0.15          # å†…å®¹ç›¸å…³æ€§
        }
        
        # è°ƒç ”å‚æ•°
        self.quality_threshold = 7.0
        self.max_papers_per_search = 100
        self.min_topic_coherence = 0.6
        
        # ç¼“å­˜å’ŒçŠ¶æ€
        self.literature_cache: Dict[str, LiteratureDocument] = {}
        self.topic_cache: Dict[str, ResearchTopic] = {}
        self.knowledge_graphs: Dict[str, KnowledgeGraph] = {}
        
        # åˆå§‹åŒ–æ–‡çŒ®æœç´¢å¼•æ“ï¼Œä½¿ç”¨ç¯å¢ƒé…ç½®
        from backend.config_env import get_env_config, set_env_variables
        # ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®
        env_config = set_env_variables()
        self.search_engine = LiteratureSearchEngine(config=env_config)
        self.search_databases = ["PubMed", "arXiv", "CrossRef", "GoogleScholar"]
        
        logger.info(f"ğŸ”§ æ–‡çŒ®æœç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨APIs: {env_config.get_available_apis()}")
        
        # è®¾ç½®Agentç±»å‹
        self.agent_type = "information_gatherer"
        self.specializations = ["literature_search", "knowledge_graph", "research_analysis"]

    async def _process_task_impl(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """å®ç°BaseAgentè¦æ±‚çš„ä»»åŠ¡å¤„ç†æ–¹æ³•"""
        try:
            # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©å¤„ç†æ–¹æ³•
            task_type = task_data.get("task_type", "literature_search")
            
            if task_type == "literature_search":
                # æ‰§è¡Œæ–‡çŒ®æœç´¢
                keywords = task_data.get("keywords", [])
                if isinstance(keywords, str):
                    keywords = [keywords]
                
                documents = await self._parallel_database_search(keywords)
                
                return {
                    "task_type": task_type,
                    "documents": documents,
                    "document_count": len(documents),
                    "keywords_used": keywords
                }
            elif task_type == "research_analysis":
                # æ‰§è¡Œç ”ç©¶åˆ†æ
                method = task_data.get("method", "hybrid")
                result = await self._execute_literature_research(task_data, method)
                return result
            else:
                # é»˜è®¤å¤„ç†
                return {
                    "task_type": task_type,
                    "status": "completed",
                    "message": f"å¤„ç†äº†{task_type}ç±»å‹çš„ä»»åŠ¡"
                }
                
        except Exception as e:
            return {
                "task_type": task_data.get("task_type", "unknown"),
                "status": "failed",
                "error": str(e)
            }

    async def _load_prompt_templates(self):
        """åŠ è½½Promptæ¨¡æ¿"""
        self.prompt_templates = {
            "keyword_extraction": """
ç³»ç»Ÿï¼šä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡çŒ®è°ƒç ”ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹ç ”ç©¶éœ€æ±‚ä¸­æå–3-5ä¸ªæ ¸å¿ƒå…³é”®è¯ã€‚

ç ”ç©¶éœ€æ±‚ï¼š{research_request}
ç ”ç©¶é¢†åŸŸï¼š{domain}
ç ”ç©¶ç›®æ ‡ï¼š{objectives}

è¦æ±‚ï¼š
1. å…³é”®è¯åº”è¯¥æ˜¯å­¦æœ¯æœ¯è¯­æˆ–æŠ€æœ¯åè¯
2. å…·æœ‰è¾ƒå¼ºçš„æ£€ç´¢èƒ½åŠ›å’Œä»£è¡¨æ€§
3. è¦†ç›–ä¸»è¦ç ”ç©¶æ–¹å‘
4. é¿å…è¿‡äºå®½æ³›æˆ–è¿‡äºå…·ä½“
5. è€ƒè™‘ä¸­è‹±æ–‡è¡¨è¾¾

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "core_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
    "expanded_keywords": {{
        "å…³é”®è¯1": ["åŒä¹‰è¯1", "ç›¸å…³è¯1"],
        "å…³é”®è¯2": ["åŒä¹‰è¯2", "ç›¸å…³è¯2"]
    }},
    "search_strategy": "å…³é”®è¯ç»„åˆæ£€ç´¢ç­–ç•¥æè¿°"
}}
""",
            
            "literature_quality_assessment": """
ç³»ç»Ÿï¼šä½ æ˜¯æ–‡çŒ®è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æ–‡çŒ®çš„è´¨é‡å’Œç›¸å…³æ€§ã€‚

æ–‡çŒ®ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
ä½œè€…ï¼š{authors}
æœŸåˆŠï¼š{journal}
å¹´ä»½ï¼š{year}
æ‘˜è¦ï¼š{abstract}
å¼•ç”¨æ¬¡æ•°ï¼š{citation_count}

            è¯„ä¼°ç»´åº¦ï¼š
1. ç ”ç©¶æ–¹æ³•è´¨é‡ (0-10åˆ†)
2. å†…å®¹ç›¸å…³æ€§ (0-10åˆ†)  
3. å­¦æœ¯æƒå¨æ€§ (0-10åˆ†)
4. åˆ›æ–°æ€§ (0-10åˆ†)
5. å¯ä¿¡åº¦ (0-10åˆ†)

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "quality_scores": {{
        "methodology": 8.5,
        "relevance": 9.0,
        "authority": 8.0,
        "innovation": 7.5,
        "credibility": 8.5
    }},
    "overall_score": 8.3,
    "assessment_summary": "è¯„ä¼°æ€»ç»“",
    "key_contributions": ["è´¡çŒ®1", "è´¡çŒ®2"],
    "limitations": ["å±€é™1", "å±€é™2"]
}}
""",

            "topic_discovery": """
ç³»ç»Ÿï¼šä½ æ˜¯ä¸»é¢˜å»ºæ¨¡ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡çŒ®é›†åˆä¸­å‘ç°æ½œåœ¨çš„ç ”ç©¶ä¸»é¢˜ã€‚

æ–‡çŒ®æ‘˜è¦é›†åˆï¼š{abstracts}

åˆ†æè¦æ±‚ï¼š
1. è¯†åˆ«3-5ä¸ªä¸»è¦ç ”ç©¶ä¸»é¢˜
2. ä¸ºæ¯ä¸ªä¸»é¢˜æä¾›æè¿°æ€§æ ‡é¢˜
3. åˆ—å‡ºæ¯ä¸ªä¸»é¢˜çš„å…³é”®æ¦‚å¿µ
4. è¯„ä¼°ä¸»é¢˜çš„ç ”ç©¶çƒ­åº¦å’Œå‘å±•è¶‹åŠ¿
5. è¯†åˆ«è·¨ä¸»é¢˜çš„è”ç³»

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "discovered_topics": [
        {{
            "topic_id": "topic_1",
            "topic_name": "ä¸»é¢˜åç§°",
            "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
            "description": "ä¸»é¢˜æè¿°",
            "document_count": 15,
            "trend": "emerging/stable/declining",
            "coherence_score": 0.85
        }}
    ],
    "topic_connections": [
        {{
            "topic1": "topic_1",
            "topic2": "topic_2", 
            "connection_strength": 0.7,
            "connection_type": "methodological/conceptual"
        }}
    ],
    "research_landscape": "ç ”ç©¶å…¨æ™¯åˆ†æ"
}}
""",

            "knowledge_graph_construction": """
ç³»ç»Ÿï¼šä½ æ˜¯çŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚è¯·ä»æ–‡çŒ®ä¿¡æ¯ä¸­æ„å»ºçŸ¥è¯†å›¾è°±ã€‚

æ–‡çŒ®æ•°æ®ï¼š{literature_data}

æ„å»ºè¦æ±‚ï¼š
1. è¯†åˆ«æ ¸å¿ƒæ¦‚å¿µå’Œå®ä½“
2. æ„å»ºæ¦‚å¿µé—´çš„å…³ç³»
3. è®¡ç®—æ¦‚å¿µçš„é‡è¦æ€§æƒé‡
4. è¯†åˆ«çŸ¥è¯†é›†ç¾¤å’Œæ¡¥æ¥æ¦‚å¿µ

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "knowledge_graph": {{
        "nodes": [
            {{
                "id": "concept_1",
                "label": "æ¦‚å¿µåç§°",
                "type": "concept/method/application",
                "importance": 0.9,
                "description": "æ¦‚å¿µæè¿°"
            }}
        ],
        "edges": [
            {{
                "source": "concept_1",
                "target": "concept_2",
                "relation": "relates_to/depends_on/enables",
                "strength": 0.8
            }}
        ]
    }},
    "central_concepts": ["æ ¸å¿ƒæ¦‚å¿µ1", "æ ¸å¿ƒæ¦‚å¿µ2"],
    "knowledge_clusters": ["é›†ç¾¤1", "é›†ç¾¤2"],
    "bridging_concepts": ["æ¡¥æ¥æ¦‚å¿µ1"]
}}
""",

            "research_trend_analysis": """
ç³»ç»Ÿï¼šä½ æ˜¯ç ”ç©¶è¶‹åŠ¿åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ–‡çŒ®çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œè¯†åˆ«ç ”ç©¶è¶‹åŠ¿ã€‚

æ–‡çŒ®æ—¶é—´æ•°æ®ï¼š{temporal_data}
å…³é”®è¯æ¼”å˜ï¼š{keyword_evolution}

åˆ†æè¦æ±‚ï¼š
1. è¯†åˆ«ç ”ç©¶çƒ­ç‚¹çš„å…´èµ·å’Œè¡°è½
2. é¢„æµ‹æœªæ¥ç ”ç©¶æ–¹å‘
3. è¯†åˆ«æ–°å…´æŠ€æœ¯å’Œæ–¹æ³•
4. åˆ†æç ”ç©¶é‡ç‚¹çš„è½¬ç§»

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "trend_analysis": {{
        "emerging_trends": ["æ–°å…´è¶‹åŠ¿1", "æ–°å…´è¶‹åŠ¿2"],
        "declining_trends": ["è¡°è½è¶‹åŠ¿1", "è¡°è½è¶‹åŠ¿2"],
        "stable_areas": ["ç¨³å®šé¢†åŸŸ1", "ç¨³å®šé¢†åŸŸ2"],
        "future_directions": ["æœªæ¥æ–¹å‘1", "æœªæ¥æ–¹å‘2"]
    }},
    "hotspot_evolution": [
        {{
            "period": "2020-2022",
            "hotspots": ["çƒ­ç‚¹1", "çƒ­ç‚¹2"],
            "intensity": 0.8
        }}
    ],
    "prediction_confidence": 0.75,
    "recommendation": "åŸºäºè¶‹åŠ¿åˆ†æçš„è°ƒç ”å»ºè®®"
}}
"""
        }

    async def _process_event_impl(self, event: BlackboardEvent) -> Any:
        """å¤„ç†ä¿¡æ¯è·å–ç›¸å…³äº‹ä»¶"""
        try:
            # è®°å½•äº‹ä»¶å¤„ç†æ¨ç†æ­¥éª¤
            event_step = ReasoningStep(
                agent_id=self.agent_id,
                step_type="event_processing",
                description=f"å¤„ç†{event.event_type.value}äº‹ä»¶",
                input_data={"event_type": event.event_type.value, "source_agent": event.agent_id},
                reasoning_text=f"ä¿¡æ¯è·å–Agentæ”¶åˆ°{event.event_type.value}äº‹ä»¶ï¼Œå¼€å§‹æ–‡çŒ®è°ƒç ”"
            )
            await self.blackboard.record_reasoning_step(event_step)
            
            if event.event_type == EventType.TASK_ASSIGNED:
                if event.target_agent == self.agent_id or event.data.get("task_type") == "information_enhanced":
                    return await self._handle_information_task(event)
            elif event.event_type == EventType.TASK_CREATED:
                return await self._handle_research_task(event.data)
            elif event.event_type == EventType.LITERATURE_SEARCH_REQUEST:
                return await self._handle_literature_search_request(event.data)
            elif event.event_type == EventType.DESIGN_REQUEST:
                return await self._handle_information_design_request(event.data)
            else:
                self.logger.warning(f"æœªå¤„ç†çš„äº‹ä»¶ç±»å‹: {event.event_type}")

        except Exception as e:
            self.logger.error(f"ä¿¡æ¯è·å–å¤„ç†å¤±è´¥: {e}")
            await self._publish_error_event(event, str(e))

    async def _handle_information_task(self, event: BlackboardEvent):
        """å¤„ç†ä¿¡æ¯è·å–ä»»åŠ¡åˆ†é…"""
        task_data = event.data
        session_id = event.session_id or "default"
        
        self.logger.info(f"å¼€å§‹ä¿¡æ¯è·å–ä»»åŠ¡: {task_data.get('user_input', '')[:50]}...")
        
        # è®°å½•ä»»åŠ¡å¼€å§‹æ¨ç†æ­¥éª¤
        task_start_step = ReasoningStep(
            agent_id=self.agent_id,
            step_type="task_start",
            description="å¼€å§‹ä¿¡æ¯è·å–å’Œæ–‡çŒ®è°ƒç ”ä»»åŠ¡",
            input_data=task_data,
            reasoning_text="æ”¶åˆ°ä¿¡æ¯è·å–ä»»åŠ¡ï¼Œå¼€å§‹æ‰§è¡Œæ™ºèƒ½æ–‡çŒ®è°ƒç ”å’ŒçŸ¥è¯†å›¾è°±æ„å»º"
        )
        await self.blackboard.record_reasoning_step(task_start_step)
        
        try:
            # é€‰æ‹©æœ€é€‚åˆçš„è°ƒç ”æ–¹æ³•
            research_method = await self._select_optimal_research_method(task_data, session_id)
            
            # è®°å½•æ–¹æ³•é€‰æ‹©æ¨ç†æ­¥éª¤
            method_step = ReasoningStep(
                agent_id=self.agent_id,
                step_type="decision",
                description=f"é€‰æ‹©{research_method}è°ƒç ”æ–¹æ³•",
                input_data={"selected_method": research_method},
                reasoning_text=f"æ ¹æ®ä»»åŠ¡å¤æ‚åº¦å’Œéœ€æ±‚åˆ†æï¼Œé€‰æ‹©{research_method}ä½œä¸ºæœ€ä¼˜è°ƒç ”ç­–ç•¥",
                confidence=0.8
            )
            await self.blackboard.record_reasoning_step(method_step)
            
            # æ‰§è¡Œæ–‡çŒ®è°ƒç ”
            research_result = await self._execute_literature_research(task_data, research_method)
            
            # å¢å¼ºRAGåŠŸèƒ½å¤„ç†
            rag_enhanced_result = await self._apply_rag_enhancement(research_result, task_data, session_id)
            
            # å‘å¸ƒä¿¡æ¯æ›´æ–°äº‹ä»¶
            await self.blackboard.publish_event(BlackboardEvent(
                event_type=EventType.INFORMATION_UPDATE,
                agent_id=self.agent_id,
                session_id=session_id,
                data={
                    "research_result": rag_enhanced_result,
                    "method": research_method,
                    "task_completed": True,
                    "timestamp": datetime.now().isoformat()
                }
            ))
            
            # è®°å½•ä»»åŠ¡å®Œæˆæ¨ç†æ­¥éª¤
            completion_step = ReasoningStep(
                agent_id=self.agent_id,
                step_type="completion",
                description="ä¿¡æ¯è·å–ä»»åŠ¡å®Œæˆ",
                input_data=task_data,
                output_data={
                    "documents_found": len(rag_enhanced_result.get("literature_documents", [])),
                    "knowledge_graph_nodes": len(rag_enhanced_result.get("knowledge_graph", {}).get("nodes", []))
                },
                reasoning_text=f"æˆåŠŸå®Œæˆæ–‡çŒ®è°ƒç ”ï¼Œè·å¾—{len(rag_enhanced_result.get('literature_documents', []))}ç¯‡é«˜è´¨é‡æ–‡çŒ®",
                confidence=0.9
            )
            await self.blackboard.record_reasoning_step(completion_step)
            
            self.logger.info(f"ä¿¡æ¯è·å–å®Œæˆï¼Œè·å¾—{len(rag_enhanced_result.get('literature_documents', []))}ç¯‡æ–‡çŒ®")
            
            return rag_enhanced_result
            
        except Exception as e:
            self.logger.error(f"ä¿¡æ¯è·å–ä»»åŠ¡å¤±è´¥: {e}")
            await self._publish_error_event(event, str(e))

    async def _select_optimal_research_method(self, task_data: Dict[str, Any], session_id: str) -> str:
        """é€‰æ‹©æœ€ä¼˜è°ƒç ”æ–¹æ³•"""
        user_input = task_data.get("user_input", "")
        
        # ç®€åŒ–çš„æ–¹æ³•é€‰æ‹©é€»è¾‘
        if "å…³é”®è¯" in user_input or "specific" in user_input.lower():
            return "keyword_driven"
        elif "ä¸»é¢˜" in user_input or "topic" in user_input.lower():
            return "topic_modeling"
        else:
            return "hybrid"

    async def _apply_rag_enhancement(self, research_result: Dict[str, Any], task_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """åº”ç”¨RAGå¢å¼ºåŠŸèƒ½"""
        # è®°å½•RAGå¤„ç†æ¨ç†æ­¥éª¤
        rag_step = ReasoningStep(
            agent_id=self.agent_id,
            step_type="enhancement",
            description="åº”ç”¨RAGå¢å¼ºå¤„ç†",
            input_data={"base_result": "research_result_summary"},
            reasoning_text="å¯¹åŸºç¡€è°ƒç ”ç»“æœåº”ç”¨RAGæŠ€æœ¯ï¼Œå¢å¼ºçŸ¥è¯†æ£€ç´¢å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›"
        )
        await self.blackboard.record_reasoning_step(rag_step)
        
        # å¢å¼ºçŸ¥è¯†å›¾è°±
        if "knowledge_graph" in research_result:
            enhanced_kg = await self._enhance_knowledge_graph_with_rag(
                research_result["knowledge_graph"], 
                task_data.get("user_input", ""),
                session_id
            )
            research_result["knowledge_graph"] = enhanced_kg
        
        # å¢å¼ºæ–‡çŒ®æ‘˜è¦å’Œå…³é”®ä¿¡æ¯æå–
        if "literature_documents" in research_result:
            enhanced_docs = await self._enhance_documents_with_rag(
                research_result["literature_documents"],
                task_data.get("user_input", ""),
                session_id
            )
            research_result["literature_documents"] = enhanced_docs
        
        # æ·»åŠ æ™ºèƒ½é—®ç­”èƒ½åŠ›
        research_result["rag_qa_capability"] = await self._build_rag_qa_system(
            research_result, task_data.get("user_input", ""), session_id
        )
        
        return research_result

    async def _enhance_knowledge_graph_with_rag(self, knowledge_graph: Dict[str, Any], user_query: str, session_id: str) -> Dict[str, Any]:
        """ä½¿ç”¨RAGæŠ€æœ¯å¢å¼ºçŸ¥è¯†å›¾è°±"""
        # åŸºäºç”¨æˆ·æŸ¥è¯¢å¢å¼ºèŠ‚ç‚¹å’Œè¾¹çš„ç›¸å…³æ€§è¯„åˆ†
        enhanced_nodes = []
        for node in knowledge_graph.get("nodes", []):
            # è®¡ç®—èŠ‚ç‚¹ä¸æŸ¥è¯¢çš„è¯­ä¹‰ç›¸ä¼¼åº¦
            relevance_score = await self._calculate_semantic_relevance(node.get("name", ""), user_query)
            node["query_relevance"] = relevance_score
            enhanced_nodes.append(node)
        
        knowledge_graph["nodes"] = enhanced_nodes
        knowledge_graph["rag_enhanced"] = True
        knowledge_graph["query_context"] = user_query
        
        return knowledge_graph

    async def _enhance_documents_with_rag(self, documents: List[Any], user_query: str, session_id: str) -> List[Any]:
        """ä½¿ç”¨RAGæŠ€æœ¯å¢å¼ºæ–‡çŒ®æ–‡æ¡£"""
        enhanced_docs = []
        
        for doc in documents:
            if hasattr(doc, 'abstract') or isinstance(doc, dict):
                # ç”ŸæˆåŸºäºæŸ¥è¯¢çš„å…³é”®ä¿¡æ¯æ‘˜è¦
                key_insights = await self._extract_query_relevant_insights(doc, user_query)
                
                if isinstance(doc, dict):
                    doc["rag_insights"] = key_insights
                    doc["query_relevance"] = await self._calculate_semantic_relevance(
                        doc.get("abstract", ""), user_query
                    )
                else:
                    # å¦‚æœæ˜¯LiteratureDocumentå¯¹è±¡ï¼Œè½¬æ¢ä¸ºdict
                    doc_dict = {
                        "doc_id": doc.doc_id,
                        "title": doc.title,
                        "authors": doc.authors,
                        "abstract": doc.abstract,
                        "keywords": doc.keywords,
                        "quality_score": doc.quality_score,
                        "rag_insights": key_insights,
                        "query_relevance": await self._calculate_semantic_relevance(doc.abstract, user_query)
                    }
                    doc = doc_dict
                
                enhanced_docs.append(doc)
        
        return enhanced_docs

    async def _build_rag_qa_system(self, research_result: Dict[str, Any], user_query: str, session_id: str) -> Dict[str, Any]:
        """æ„å»ºRAGé—®ç­”ç³»ç»Ÿ"""
        # æ„å»ºçŸ¥è¯†åº“
        knowledge_base = []
        
        # ä»æ–‡çŒ®ä¸­æå–çŸ¥è¯†
        for doc in research_result.get("literature_documents", []):
            knowledge_base.append({
                "source": "literature",
                "content": doc.get("abstract", "") if isinstance(doc, dict) else doc.abstract,
                "title": doc.get("title", "") if isinstance(doc, dict) else doc.title,
                "relevance": doc.get("query_relevance", 0) if isinstance(doc, dict) else 0
            })
        
        # ä»çŸ¥è¯†å›¾è°±ä¸­æå–çŸ¥è¯†
        kg = research_result.get("knowledge_graph", {})
        for node in kg.get("nodes", []):
            knowledge_base.append({
                "source": "knowledge_graph",
                "content": node.get("description", ""),
                "entity": node.get("name", ""),
                "relevance": node.get("query_relevance", 0)
            })
        
        return {
            "knowledge_base": knowledge_base,
            "query_capabilities": ["factual_qa", "conceptual_explanation", "comparison_analysis"],
            "supported_question_types": ["What", "How", "Why", "Compare", "Explain"],
            "ready": True
        }

    async def _calculate_semantic_relevance(self, text: str, query: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸å…³æ€§ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # ç®€åŒ–çš„ç›¸å…³æ€§è®¡ç®—ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹
        text_lower = text.lower()
        query_lower = query.lower()
        
        # åŸºäºå…³é”®è¯é‡å çš„ç®€å•ç›¸å…³æ€§
        query_words = set(query_lower.split())
        text_words = set(text_lower.split())
        
        overlap = len(query_words.intersection(text_words))
        relevance = overlap / max(len(query_words), 1)
        
        return min(relevance * 2, 1.0)  # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´

    async def _extract_query_relevant_insights(self, doc: Any, user_query: str) -> List[str]:
        """æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®æ´å¯Ÿ"""
        # ç®€åŒ–çš„æ´å¯Ÿæå–
        insights = []
        
        doc_text = doc.get("abstract", "") if isinstance(doc, dict) else getattr(doc, 'abstract', "")
        
        # åŸºäºæŸ¥è¯¢å…³é”®è¯æå–ç›¸å…³å¥å­
        sentences = doc_text.split('.')
        query_words = set(user_query.lower().split())
        
        for sentence in sentences:
            sentence_words = set(sentence.lower().split())
            if len(query_words.intersection(sentence_words)) >= 1:
                insights.append(sentence.strip())
        
        return insights[:3]  # è¿”å›æœ€å¤š3ä¸ªå…³é”®æ´å¯Ÿ

    async def _handle_research_task(self, task_data: Dict[str, Any]) -> None:
        """å¤„ç†ç ”ç©¶ä»»åŠ¡"""
        task_type = task_data.get("task_type", "")
        
        if task_type not in ["information_retrieval", "literature_search", "keyword_driven_search", 
                           "topic_modeling_search", "hybrid_research"]:
            return

        self.logger.info(f"å¼€å§‹å¤„ç†ä¿¡æ¯è·å–ä»»åŠ¡: {task_type}")
        
        # ç¡®å®šè°ƒç ”æ–¹æ³•
        if task_type == "keyword_driven_search":
            method = "keyword_driven"
        elif task_type == "topic_modeling_search":
            method = "topic_modeling"
        else:
            method = "hybrid"
        
        # æ‰§è¡Œè°ƒç ”
        research_result = await self._execute_literature_research(task_data, method)
        
        # å‘å¸ƒè°ƒç ”å®Œæˆäº‹ä»¶
        await self.publish_result(
            EventType.ENHANCED_INFORMATION_RESEARCH_COMPLETED,
            {
                "task_id": task_data.get("task_id"),
                "research_result": research_result,
                "method": method,
                "session_id": task_data.get("session_id")
            }
        )

    async def _execute_literature_research(self, task_data: Dict[str, Any], method: str) -> Dict[str, Any]:
        """æ‰§è¡Œæ–‡çŒ®è°ƒç ”"""
        self.logger.info(f"æ‰§è¡Œ{self.research_methods[method]}")
        
        if method == "keyword_driven":
            return await self._keyword_driven_research(task_data)
        elif method == "topic_modeling":
            return await self._topic_modeling_research(task_data)
        else:  # hybrid
            return await self._hybrid_research(task_data)

    async def _keyword_driven_research(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ–¹æ³•ä¸€ï¼šå…³é”®è¯é©±åŠ¨æ–‡çŒ®æ£€ç´¢æ–¹æ³•"""
        self.logger.info("æ‰§è¡Œå…³é”®è¯é©±åŠ¨æ–‡çŒ®è°ƒç ”")
        
        try:
            # 1. å…³é”®è¯æå–å’Œæ‰©å±•
            keywords_result = await self._extract_and_expand_keywords(task_data)
            
            # 2. å¤šæ•°æ®åº“å¹¶è¡Œæ£€ç´¢
            search_results = await self._parallel_database_search(keywords_result["core_keywords"])
            
            # 3. æ–‡çŒ®è´¨é‡è¯„ä¼°å’Œç­›é€‰
            filtered_literature = await self._quality_assessment_and_filtering(search_results)
            
            # 4. æ„å»ºçŸ¥è¯†å›¾è°±
            knowledge_graph = await self._enhanced_knowledge_graph_construction(filtered_literature, task_data.get("session_id"))
            
            # 5. ç”Ÿæˆè°ƒç ”æŠ¥å‘Š
            research_report = await self._generate_keyword_driven_report(
                keywords_result, filtered_literature, knowledge_graph
            )
            
            # å‘å¸ƒå…³é”®è¯æå–å®Œæˆäº‹ä»¶
            await self.publish_result(
                EventType.KEYWORD_EXTRACTION_COMPLETED,
                {
                    "keywords": keywords_result,
                    "literature_count": len(filtered_literature)
                }
            )
            
            # å‘å¸ƒçŸ¥è¯†å›¾è°±æ›´æ–°äº‹ä»¶
            await self.publish_result(
                EventType.KNOWLEDGE_GRAPH_UPDATED,
                {
                    "knowledge_graph": knowledge_graph,
                    "method": "keyword_driven"
                }
            )
            
            return {
                "method": "keyword_driven",
                "keywords_result": keywords_result,
                "literature_documents": filtered_literature,
                "knowledge_graph": knowledge_graph,
                "research_report": research_report,
                "performance_metrics": {
                    "documents_retrieved": len(search_results),
                    "documents_after_filtering": len(filtered_literature),
                    "precision": len(filtered_literature) / max(len(search_results), 1),
                    "avg_quality_score": np.mean([doc.quality_score for doc in filtered_literature]) if filtered_literature else 0
                }
            }

        except Exception as e:
            self.logger.error(f"å…³é”®è¯é©±åŠ¨è°ƒç ”å¤±è´¥: {e}")
            return {"error": str(e), "method": "keyword_driven"}

    async def _topic_modeling_research(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ–¹æ³•äºŒï¼šä¸»é¢˜å»ºæ¨¡æ™ºèƒ½å‘ç°æ–¹æ³•"""
        self.logger.info("æ‰§è¡Œä¸»é¢˜å»ºæ¨¡æ–‡çŒ®è°ƒç ”")
        
        try:
            # 1. æ”¶é›†ç§å­æ–‡çŒ®
            seed_documents = await self._collect_seed_documents(task_data)
            
            # 2. ä¸»é¢˜å»ºæ¨¡å’Œå‘ç°
            discovered_topics = await self._discover_research_topics(seed_documents)
            
            # 3. åŸºäºä¸»é¢˜çš„æ‰©å±•æ£€ç´¢
            expanded_literature = await self._topic_based_expansion_search(discovered_topics)
            
            # 4. è¯­ä¹‰èšç±»åˆ†æ
            literature_clusters = await self._perform_semantic_clustering(expanded_literature)
            
            # 5. ç ”ç©¶è¶‹åŠ¿åˆ†æ
            trend_analysis = await self._analyze_research_trends(expanded_literature)
            
            # 6. ä¸“å®¶çŸ¥è¯†éªŒè¯
            validated_results = await self._expert_knowledge_validation(
                discovered_topics, literature_clusters, trend_analysis
            )
            
            # 7. ç”Ÿæˆæ™ºèƒ½è°ƒç ”æŠ¥å‘Š
            intelligent_report = await self._generate_topic_modeling_report(validated_results)
            
            # å‘å¸ƒä¸»é¢˜å»ºæ¨¡å®Œæˆäº‹ä»¶
            await self.publish_result(
                EventType.TOPIC_MODELING_COMPLETED,
                {
                    "discovered_topics": discovered_topics,
                    "literature_clusters": literature_clusters
                }
            )
            
            # å‘å¸ƒç ”ç©¶è¶‹åŠ¿è¯†åˆ«äº‹ä»¶
            await self.publish_result(
                EventType.RESEARCH_TREND_IDENTIFIED,
                {
                    "trend_analysis": trend_analysis,
                    "method": "topic_modeling"
                }
            )
            
            return {
                "method": "topic_modeling",
                "discovered_topics": discovered_topics,
                "literature_clusters": literature_clusters,
                "trend_analysis": trend_analysis,
                "validated_results": validated_results,
                "intelligent_report": intelligent_report,
                "performance_metrics": {
                    "topics_discovered": len(discovered_topics),
                    "literature_processed": len(expanded_literature),
                    "cluster_coherence": np.mean([topic.coherence_score for topic in discovered_topics]),
                    "trend_confidence": trend_analysis.get("prediction_confidence", 0)
                }
            }
            
        except Exception as e:
            self.logger.error(f"ä¸»é¢˜å»ºæ¨¡è°ƒç ”å¤±è´¥: {e}")
            return {"error": str(e), "method": "topic_modeling"}

    async def _hybrid_research(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ–¹æ³•ä¸‰ï¼šæ··åˆæ¨¡å¼è°ƒç ”æ–¹æ³•"""
        self.logger.info("æ‰§è¡Œæ··åˆæ¨¡å¼æ–‡çŒ®è°ƒç ”")
        
        try:
            # å¹¶è¡Œæ‰§è¡Œä¸¤ç§æ–¹æ³•
            keyword_task = asyncio.create_task(self._keyword_driven_research(task_data))
            topic_task = asyncio.create_task(self._topic_modeling_research(task_data))
            
            keyword_result, topic_result = await asyncio.gather(keyword_task, topic_task)
            
            # ç»“æœèåˆ
            merged_results = await self._merge_research_results(keyword_result, topic_result)
            
            # äº¤å‰éªŒè¯
            cross_validated_results = await self._cross_validate_results(merged_results)
            
            # ç»¼åˆè´¨é‡è¯„ä¼°
            comprehensive_quality = await self._comprehensive_quality_assessment(cross_validated_results)
            
            # ç”Ÿæˆæ··åˆè°ƒç ”æŠ¥å‘Š
            hybrid_report = await self._generate_hybrid_research_report(cross_validated_results)
            
            # å‘å¸ƒäº¤å‰éªŒè¯å®Œæˆäº‹ä»¶
            await self.publish_result(
                EventType.CROSS_VALIDATION_COMPLETED,
                {
                    "validation_results": cross_validated_results,
                    "quality_assessment": comprehensive_quality
                }
            )
            
            return {
                "method": "hybrid",
                "keyword_driven_result": keyword_result,
                "topic_modeling_result": topic_result,
                "merged_results": merged_results,
                "cross_validated_results": cross_validated_results,
                "comprehensive_quality": comprehensive_quality,
                "hybrid_report": hybrid_report,
                "performance_metrics": {
                    "total_coverage": comprehensive_quality.get("coverage_score", 0),
                    "result_reliability": comprehensive_quality.get("reliability_score", 0),
                    "method_agreement": comprehensive_quality.get("agreement_score", 0)
                }
            }
            
        except Exception as e:
            self.logger.error(f"æ··åˆæ¨¡å¼è°ƒç ”å¤±è´¥: {e}")
            return {"error": str(e), "method": "hybrid"}

    async def _extract_and_expand_keywords(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å’Œæ‰©å±•å…³é”®è¯"""
        prompt = self.format_prompt(
            "keyword_extraction",
            research_request=task_data.get("description", ""),
            domain=task_data.get("domain", ""),
            objectives=json.dumps(task_data.get("objectives", []), ensure_ascii=False)
        )
        
        response = await self.call_llm(prompt, response_format="json")
        return json.loads(response)

    async def _parallel_database_search(self, keywords: List[str]) -> List[LiteratureDocument]:
        """å¹¶è¡Œå¤šæ•°æ®åº“æ£€ç´¢ï¼ˆçœŸå®APIï¼‰"""
        search_tasks = []
        for database in self.search_databases:
            for keyword in keywords:
                task = asyncio.create_task(self._search_single_database(database, keyword))
                search_tasks.append(task)
        search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        # åˆå¹¶å’Œå»é‡
        all_documents = []
        seen_titles = set()
        for result in search_results:
            if isinstance(result, list):
                for doc in result:
                    if doc.title not in seen_titles:
                        all_documents.append(doc)
                        seen_titles.add(doc.title)
        return all_documents

    async def _search_single_database(self, database: str, keyword: str) -> List[LiteratureDocument]:
        """å•ä¸ªæ•°æ®åº“æ£€ç´¢ï¼ˆçœŸå®APIï¼‰"""
        limit = 20
        try:
            # åˆ›å»ºSearchQueryå¯¹è±¡
            from backend.utils.literature_search import SearchQuery
            query = SearchQuery(keywords=[keyword], max_results=limit)
            
            if database == "PubMed":
                logger.info(f"ğŸ” æœç´¢PubMed: {keyword}")
                results = await self.search_engine.search_pubmed(query)
            elif database == "arXiv":
                logger.info(f"ğŸ” æœç´¢arXiv: {keyword}")
                results = await self.search_engine.search_arxiv(query)
            elif database == "CrossRef":
                logger.info(f"ğŸ” æœç´¢CrossRef: {keyword}")
                results = await self.search_engine.search_crossref(query)
            elif database == "GoogleScholar":
                logger.info(f"ğŸ” æœç´¢GoogleScholar: {keyword}")
                try:
                    # ä¼˜å…ˆä½¿ç”¨SearchApiï¼Œå›é€€åˆ°SerpApi
                    results = await self.search_engine.search_searchapi_google_scholar(query)
                    if not results:
                        results = await self.search_engine.search_serpapi_google_scholar(query)
                except Exception as e:
                    logger.error(f"GoogleScholar APIå¼‚å¸¸: {e}")
                    results = []
            else:
                logger.warning(f"æœªçŸ¥æ•°æ®åº“: {database}")
                results = []
                
            if not results:
                logger.info(f"ğŸ“­ {database} æœªæ‰¾åˆ°å…³é”®è¯ '{keyword}' çš„ç›¸å…³æ–‡çŒ®")
            else:
                logger.info(f"âœ… {database} æ£€ç´¢æˆåŠŸ: æ‰¾åˆ° {len(results)} ç¯‡æ–‡çŒ®")
                
        except Exception as e:
            logger.error(f"{database} æ£€ç´¢å¤±è´¥: {e}")
            results = []

        # è½¬æ¢ä¸º LiteratureDocument
        docs = []
        logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ {len(results)} æ¡ {database} æ£€ç´¢ç»“æœ...")
        
        for i, item in enumerate(results):
            try:
                # å®‰å…¨æå–å¹´ä»½
                year = 0
                if hasattr(item, 'publication_date') and item.publication_date:
                    year_str = str(item.publication_date)[:4]
                    if year_str.isdigit():
                        year = int(year_str)
                
                doc = LiteratureDocument(
                    doc_id=f"doc_{database}_{keyword}_{i}",
                    title=getattr(item, 'title', '') or '',
                    authors=getattr(item, 'authors', []) or [],
                    journal=getattr(item, 'journal', '') or '',
                    year=year,
                    abstract=getattr(item, 'abstract', '') or '',
                    keywords=[keyword],
                    citation_count=getattr(item, 'citation_count', 0) or 0,
                    journal_impact_factor=0.0,
                    relevance_score=0.0,
                    quality_score=0.0,
                    source_database=getattr(item, 'source_database', database) or database,
                    doi=getattr(item, 'doi', '') or '',
                    full_text=""
                )
                docs.append(doc)
                logger.debug(f"âœ… è½¬æ¢æ–‡æ¡£ {i+1}: {doc.title[:50]}...")
                
            except Exception as e:
                logger.error(f"âŒ è½¬æ¢ç¬¬ {i+1} æ¡æ–‡æ¡£å¤±è´¥: {e}")
                logger.error(f"   åŸå§‹æ•°æ®ç±»å‹: {type(item)}")
                logger.error(f"   åŸå§‹æ•°æ®: {str(item)[:200]}...")
                continue
        
        logger.info(f"ğŸ“š {database} æˆåŠŸè½¬æ¢ {len(docs)} æ¡æ–‡çŒ®æ–‡æ¡£")
        return docs

    async def _quality_assessment_and_filtering(self, documents: List[LiteratureDocument]) -> List[LiteratureDocument]:
        """æ–‡çŒ®è´¨é‡è¯„ä¼°å’Œç­›é€‰"""
        assessed_documents = []
        
        for doc in documents:
            try:
                # è°ƒç”¨LLMè¿›è¡Œè´¨é‡è¯„ä¼°
                assessment = await self._assess_single_document_quality(doc)
                
                # æ›´æ–°æ–‡æ¡£çš„è´¨é‡åˆ†æ•°
                doc.quality_score = assessment["overall_score"]
                doc.relevance_score = assessment["quality_scores"]["relevance"]
                
                # æ ¹æ®è´¨é‡é˜ˆå€¼ç­›é€‰
                if doc.quality_score >= self.quality_threshold:
                    assessed_documents.append(doc)
                    
            except Exception as e:
                self.logger.warning(f"æ–‡æ¡£è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        # å‘å¸ƒæ–‡çŒ®è´¨é‡è¯„ä¼°å®Œæˆäº‹ä»¶
        await self.publish_result(
            EventType.LITERATURE_QUALITY_ASSESSED,
            {
                "total_documents": len(documents),
                "qualified_documents": len(assessed_documents),
                "avg_quality_score": np.mean([doc.quality_score for doc in assessed_documents]) if assessed_documents else 0
            }
        )
        
        return assessed_documents

    async def _assess_single_document_quality(self, doc: LiteratureDocument) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ–‡æ¡£è´¨é‡"""
        prompt = self.format_prompt(
            "literature_quality_assessment",
            title=doc.title,
            authors=", ".join(doc.authors),
            journal=doc.journal,
            year=doc.year,
            abstract=doc.abstract,
            citation_count=doc.citation_count
        )
        
        response = await self.call_llm(prompt, response_format="json")
        return json.loads(response)

    async def _enhanced_knowledge_graph_construction(self, documents: List[LiteratureDocument], 
                                                   session_id: str) -> KnowledgeGraph:
        """å¢å¼ºçš„çŸ¥è¯†å›¾è°±æ„å»º"""
        try:
            # 1. æ¦‚å¿µæå–å’Œå®ä½“è¯†åˆ«
            entities = await self._extract_scientific_entities(documents, session_id)
            
            # 2. å…³ç³»æŒ–æ˜å’Œè¯­ä¹‰è¿æ¥
            relationships = await self._mine_semantic_relationships(entities, documents, session_id)
            
            # 3. è·¨å­¦ç§‘è¿æ¥å‘ç°
            interdisciplinary_connections = await self._discover_interdisciplinary_connections(
                entities, relationships, session_id
            )
            
            # 4. åˆ›æ–°æœºä¼šè¯†åˆ«
            innovation_opportunities = await self._identify_innovation_opportunities(
                entities, relationships, interdisciplinary_connections, session_id
            )
            
            # 5. æ„å»ºå¢å¼ºçŸ¥è¯†å›¾è°±
            enhanced_kg = await self._build_enhanced_knowledge_graph(
                entities, relationships, interdisciplinary_connections, 
                innovation_opportunities, session_id
            )
            
            # è®°å½•çŸ¥è¯†å›¾è°±æ„å»ºè¿‡ç¨‹
            await self._record_llm_chain_step(
                f"å¢å¼ºçŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: å®ä½“æ•°é‡ {len(entities)}",
                f"å…³ç³»æ•°é‡: {len(relationships)}, è·¨å­¦ç§‘è¿æ¥: {len(interdisciplinary_connections)}, åˆ›æ–°æœºä¼š: {len(innovation_opportunities)}",
                session_id
            )
            
            return enhanced_kg
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºçŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {e}")
            return await self._build_knowledge_graph(documents)

    async def _extract_scientific_entities(self, documents: List[LiteratureDocument], 
                                         session_id: str) -> List[Dict[str, Any]]:
        """æå–ç§‘å­¦å®ä½“å’Œæ¦‚å¿µ"""
        try:
            entities = []
            
            for doc in documents[:10]:  # å¤„ç†å‰10ä¸ªæ–‡æ¡£ä»¥æé«˜æ•ˆç‡
                extraction_prompt = f"""
                ä»ä»¥ä¸‹ç§‘å­¦æ–‡çŒ®ä¸­æå–å…³é”®å®ä½“å’Œæ¦‚å¿µï¼š
                
                æ ‡é¢˜ï¼š{doc.title}
                æ‘˜è¦ï¼š{doc.abstract}
                å…³é”®è¯ï¼š{', '.join(doc.keywords)}
                
                è¯·æå–ä»¥ä¸‹ç±»å‹çš„å®ä½“ï¼š
                
                1. ç§‘å­¦æ¦‚å¿µ (Scientific Concepts)
                2. ç ”ç©¶æ–¹æ³• (Research Methods)
                3. æŠ€æœ¯æœ¯è¯­ (Technical Terms)
                4. ææ–™/ç‰©è´¨ (Materials/Substances)
                5. ç†è®ºæ¨¡å‹ (Theoretical Models)
                6. åº”ç”¨é¢†åŸŸ (Application Domains)
                7. ç ”ç©¶é—®é¢˜ (Research Problems)
                8. åˆ›æ–°ç‚¹ (Innovation Points)
                
                å¯¹æ¯ä¸ªå®ä½“ï¼Œè¯·æä¾›ï¼š
                - å®ä½“åç§°
                - å®ä½“ç±»å‹
                - é‡è¦æ€§è¯„åˆ† (1-10)
                - ç®€çŸ­æè¿°
                - å­¦ç§‘é¢†åŸŸ
                
                ä»¥JSONæ ¼å¼è¿”å›å®ä½“åˆ—è¡¨ã€‚
                """
                
                response = await self.call_llm(
                    extraction_prompt,
                    temperature=0.3,
                    max_tokens=1500,
                    response_format="json",
                    session_id=session_id
                )
                
                try:
                    doc_entities = json.loads(response)
                    if isinstance(doc_entities, list):
                        for entity in doc_entities:
                            entity['source_doc'] = doc.doc_id
                            entity['source_title'] = doc.title
                            entities.append(entity)
                except json.JSONDecodeError:
                    self.logger.warning(f"å®ä½“æå–å“åº”è§£æå¤±è´¥: {doc.doc_id}")
                
                await asyncio.sleep(0.1)  # é¿å…APIè°ƒç”¨è¿‡é¢‘
            
            # å»é‡å’Œèšåˆç›¸ä¼¼å®ä½“
            unique_entities = await self._deduplicate_entities(entities, session_id)
            
            return unique_entities
            
        except Exception as e:
            self.logger.error(f"ç§‘å­¦å®ä½“æå–å¤±è´¥: {e}")
            return []

    async def _mine_semantic_relationships(self, entities: List[Dict[str, Any]], 
                                         documents: List[LiteratureDocument],
                                         session_id: str) -> List[Dict[str, Any]]:
        """æŒ–æ˜è¯­ä¹‰å…³ç³»"""
        try:
            relationships = []
            
            # åŸºäºå®ä½“å…±ç°æŒ–æ˜å…³ç³»
            entity_pairs = []
            for i, entity1 in enumerate(entities):
                for j, entity2 in enumerate(entities[i+1:], i+1):
                    if entity1.get('source_doc') == entity2.get('source_doc'):
                        entity_pairs.append((entity1, entity2))
            
            # åˆ†æ‰¹å¤„ç†å®ä½“å¯¹ä»¥é¿å…è¿‡å¤šAPIè°ƒç”¨
            for batch_start in range(0, min(len(entity_pairs), 50), 10):
                batch = entity_pairs[batch_start:batch_start+10]
                
                relationship_prompt = f"""
                åˆ†æä»¥ä¸‹å®ä½“å¯¹ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼š
                
                {json.dumps([{
                    'entity1': pair[0]['name'],
                    'entity2': pair[1]['name'],
                    'type1': pair[0]['type'],
                    'type2': pair[1]['type'],
                    'domain1': pair[0].get('domain', ''),
                    'domain2': pair[1].get('domain', '')
                } for pair in batch], ensure_ascii=False, indent=2)}
                
                å¯¹æ¯ä¸ªå®ä½“å¯¹ï¼Œè¯·è¯†åˆ«ä»¥ä¸‹ç±»å‹çš„å…³ç³»ï¼š
                
                1. å› æœå…³ç³» (causal): Aå¯¼è‡´B
                2. ç»„æˆå…³ç³» (compositional): Aæ˜¯Bçš„ç»„æˆéƒ¨åˆ†
                3. åŠŸèƒ½å…³ç³» (functional): Aç”¨äºB
                4. ç›¸ä¼¼å…³ç³» (similar): Aä¸Bç›¸ä¼¼
                5. å¯¹ç«‹å…³ç³» (opposite): Aä¸Bå¯¹ç«‹
                6. ä¾èµ–å…³ç³» (dependency): Aä¾èµ–äºB
                7. åº”ç”¨å…³ç³» (application): Aåº”ç”¨äºB
                8. æ”¹è¿›å…³ç³» (improvement): Aæ”¹è¿›B
                
                å¯¹äºå­˜åœ¨å…³ç³»çš„å®ä½“å¯¹ï¼Œè¯·æä¾›ï¼š
                - å…³ç³»ç±»å‹
                - å…³ç³»å¼ºåº¦ (1-10)
                - å…³ç³»æè¿°
                - ç½®ä¿¡åº¦ (0-1)
                
                ä»¥JSONæ ¼å¼è¿”å›å…³ç³»åˆ—è¡¨ã€‚
                """
                
                response = await self.call_llm(
                    relationship_prompt,
                    temperature=0.2,
                    max_tokens=1500,
                    response_format="json",
                    session_id=session_id
                )
                
                try:
                    batch_relationships = json.loads(response)
                    if isinstance(batch_relationships, list):
                        relationships.extend(batch_relationships)
                except json.JSONDecodeError:
                    self.logger.warning(f"å…³ç³»æŒ–æ˜å“åº”è§£æå¤±è´¥")
                
                await asyncio.sleep(0.2)
            
            return relationships
            
        except Exception as e:
            self.logger.error(f"è¯­ä¹‰å…³ç³»æŒ–æ˜å¤±è´¥: {e}")
            return []

    async def _discover_interdisciplinary_connections(self, entities: List[Dict[str, Any]], 
                                                    relationships: List[Dict[str, Any]],
                                                    session_id: str) -> List[Dict[str, Any]]:
        """å‘ç°è·¨å­¦ç§‘è¿æ¥"""
        try:
            # è¯†åˆ«ä¸åŒå­¦ç§‘é¢†åŸŸçš„å®ä½“
            domains = {}
            for entity in entities:
                domain = entity.get('domain', 'unknown')
                if domain not in domains:
                    domains[domain] = []
                domains[domain].append(entity)
            
            interdisciplinary_connections = []
            
            # åˆ†æè·¨é¢†åŸŸå®ä½“é—´çš„æ½œåœ¨è¿æ¥
            domain_pairs = []
            domain_list = list(domains.keys())
            for i, domain1 in enumerate(domain_list):
                for domain2 in domain_list[i+1:]:
                    if domain1 != domain2:
                        domain_pairs.append((domain1, domain2))
            
            for domain1, domain2 in domain_pairs[:10]:  # é™åˆ¶å¤„ç†æ•°é‡
                connection_prompt = f"""
                åˆ†æä»¥ä¸‹ä¸¤ä¸ªå­¦ç§‘é¢†åŸŸä¹‹é—´çš„æ½œåœ¨è·¨å­¦ç§‘è¿æ¥ï¼š
                
                é¢†åŸŸ1: {domain1}
                ç›¸å…³å®ä½“: {[entity['name'] for entity in domains[domain1][:5]]}
                
                é¢†åŸŸ2: {domain2}
                ç›¸å…³å®ä½“: {[entity['name'] for entity in domains[domain2][:5]]}
                
                è¯·è¯†åˆ«ä»¥ä¸‹ç±»å‹çš„è·¨å­¦ç§‘è¿æ¥ï¼š
                
                1. æ–¹æ³•è¿ç§» (method_transfer): ä¸€ä¸ªé¢†åŸŸçš„æ–¹æ³•å¯åº”ç”¨äºå¦ä¸€ä¸ªé¢†åŸŸ
                2. æ¦‚å¿µèåˆ (concept_fusion): ä¸¤ä¸ªé¢†åŸŸçš„æ¦‚å¿µå¯ä»¥ç»“åˆ
                3. æŠ€æœ¯äº¤å‰ (technology_crossover): æŠ€æœ¯åœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨
                4. ç†è®ºå€Ÿé‰´ (theory_borrowing): ç†è®ºæ¡†æ¶çš„è·¨é¢†åŸŸåº”ç”¨
                5. æ•°æ®å…±äº« (data_sharing): æ•°æ®åœ¨ä¸åŒé¢†åŸŸçš„å…±åŒä»·å€¼
                6. å·¥å…·å…±ç”¨ (tool_sharing): å·¥å…·å’Œè®¾å¤‡çš„è·¨é¢†åŸŸä½¿ç”¨
                
                å¯¹æ¯ä¸ªè¿æ¥ï¼Œè¯·æä¾›ï¼š
                - è¿æ¥ç±»å‹
                - è¿æ¥æè¿°
                - åˆ›æ–°æ½œåŠ›è¯„åˆ† (1-10)
                - å®ç°éš¾åº¦è¯„åˆ† (1-10)
                - å…·ä½“åº”ç”¨åœºæ™¯
                
                ä»¥JSONæ ¼å¼è¿”å›è¿æ¥åˆ—è¡¨ã€‚
                """
                
                response = await self.call_llm(
                    connection_prompt,
                    temperature=0.4,
                    max_tokens=1500,
                    response_format="json",
                    session_id=session_id
                )
                
                try:
                    connections = json.loads(response)
                    if isinstance(connections, list):
                        for conn in connections:
                            conn['domain1'] = domain1
                            conn['domain2'] = domain2
                            interdisciplinary_connections.append(conn)
                except json.JSONDecodeError:
                    self.logger.warning(f"è·¨å­¦ç§‘è¿æ¥è§£æå¤±è´¥: {domain1} - {domain2}")
                
                await asyncio.sleep(0.3)
            
            return interdisciplinary_connections
            
        except Exception as e:
            self.logger.error(f"è·¨å­¦ç§‘è¿æ¥å‘ç°å¤±è´¥: {e}")
            return []

    async def _identify_innovation_opportunities(self, entities: List[Dict[str, Any]], 
                                               relationships: List[Dict[str, Any]],
                                               interdisciplinary_connections: List[Dict[str, Any]],
                                               session_id: str) -> List[Dict[str, Any]]:
        """è¯†åˆ«åˆ›æ–°æœºä¼š"""
        try:
            innovation_opportunities = []
            
            # åŸºäºå®ä½“å’Œå…³ç³»åˆ†æåˆ›æ–°æœºä¼š
            innovation_prompt = f"""
            åŸºäºä»¥ä¸‹ç§‘å­¦çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œè¯†åˆ«æ½œåœ¨çš„åˆ›æ–°æœºä¼šï¼š
            
            å…³é”®å®ä½“æ•°é‡: {len(entities)}
            å…³ç³»æ•°é‡: {len(relationships)}
            è·¨å­¦ç§‘è¿æ¥: {len(interdisciplinary_connections)}
            
            é«˜é‡è¦æ€§å®ä½“: {[e['name'] for e in entities if e.get('importance', 0) >= 8][:10]}
            
            å¼ºå…³ç³»: {[r for r in relationships if r.get('strength', 0) >= 8][:5]}
            
            é«˜æ½œåŠ›è·¨å­¦ç§‘è¿æ¥: {[c for c in interdisciplinary_connections if c.get('innovation_potential', 0) >= 8][:5]}
            
            è¯·è¯†åˆ«ä»¥ä¸‹ç±»å‹çš„åˆ›æ–°æœºä¼šï¼š
            
            1. æŠ€æœ¯èåˆæœºä¼š (technology_fusion): ä¸åŒæŠ€æœ¯çš„åˆ›æ–°æ€§ç»“åˆ
            2. æ–¹æ³•åˆ›æ–°æœºä¼š (method_innovation): æ–°æ–¹æ³•çš„å¼€å‘æœºä¼š
            3. åº”ç”¨æ‹“å±•æœºä¼š (application_extension): ç°æœ‰æŠ€æœ¯çš„æ–°åº”ç”¨
            4. ç†è®ºçªç ´æœºä¼š (theoretical_breakthrough): ç†è®ºåˆ›æ–°çš„å¯èƒ½æ€§
            5. è·¨ç•Œåˆä½œæœºä¼š (cross_domain_collaboration): è·¨é¢†åŸŸåˆä½œçš„æœºä¼š
            6. æŠ€æœ¯ç©ºç™½æœºä¼š (technology_gap): æŠ€æœ¯ç©ºç™½çš„å¡«è¡¥æœºä¼š
            
            å¯¹æ¯ä¸ªåˆ›æ–°æœºä¼šï¼Œè¯·æä¾›ï¼š
            - æœºä¼šç±»å‹
            - æœºä¼šæè¿°
            - åˆ›æ–°ç¨‹åº¦è¯„åˆ† (1-10)
            - å®ç°å¯è¡Œæ€§è¯„åˆ† (1-10)
            - å¸‚åœºæ½œåŠ›è¯„åˆ† (1-10)
            - æŠ€æœ¯éš¾åº¦è¯„åˆ† (1-10)
            - æ‰€éœ€èµ„æº
            - é¢„æœŸå½±å“
            
            ä»¥JSONæ ¼å¼è¿”å›åˆ›æ–°æœºä¼šåˆ—è¡¨ã€‚
            """
            
            response = await self.call_llm(
                innovation_prompt,
                temperature=0.5,
                max_tokens=2000,
                response_format="json",
                session_id=session_id
            )
            
            try:
                innovation_opportunities = json.loads(response)
                if not isinstance(innovation_opportunities, list):
                    innovation_opportunities = []
            except json.JSONDecodeError:
                self.logger.warning("åˆ›æ–°æœºä¼šè¯†åˆ«å“åº”è§£æå¤±è´¥")
                innovation_opportunities = []
            
            return innovation_opportunities
            
        except Exception as e:
            self.logger.error(f"åˆ›æ–°æœºä¼šè¯†åˆ«å¤±è´¥: {e}")
            return []

    async def _build_enhanced_knowledge_graph(self, entities: List[Dict[str, Any]], 
                                            relationships: List[Dict[str, Any]],
                                            interdisciplinary_connections: List[Dict[str, Any]],
                                            innovation_opportunities: List[Dict[str, Any]],
                                            session_id: str) -> KnowledgeGraph:
        """æ„å»ºå¢å¼ºçŸ¥è¯†å›¾è°±"""
        try:
            # è½¬æ¢å®ä½“ä¸ºèŠ‚ç‚¹
            nodes = []
            for entity in entities:
                node = {
                    "id": f"entity_{len(nodes)}",
                    "name": entity.get('name', ''),
                    "type": entity.get('type', 'concept'),
                    "domain": entity.get('domain', 'unknown'),
                    "importance": entity.get('importance', 5),
                    "description": entity.get('description', ''),
                    "source_docs": [entity.get('source_doc', '')]
                }
                nodes.append(node)
            
            # æ·»åŠ è·¨å­¦ç§‘è¿æ¥èŠ‚ç‚¹
            for conn in interdisciplinary_connections:
                node = {
                    "id": f"connection_{len(nodes)}",
                    "name": f"{conn.get('domain1', '')} - {conn.get('domain2', '')} è¿æ¥",
                    "type": "interdisciplinary_connection",
                    "domain": "interdisciplinary",
                    "importance": conn.get('innovation_potential', 5),
                    "description": conn.get('description', ''),
                    "connection_type": conn.get('type', '')
                }
                nodes.append(node)
            
            # æ·»åŠ åˆ›æ–°æœºä¼šèŠ‚ç‚¹
            for opp in innovation_opportunities:
                node = {
                    "id": f"opportunity_{len(nodes)}",
                    "name": f"åˆ›æ–°æœºä¼š: {opp.get('type', '')}",
                    "type": "innovation_opportunity",
                    "domain": "innovation",
                    "importance": opp.get('innovation_score', 5),
                    "description": opp.get('description', ''),
                    "feasibility": opp.get('feasibility', 5),
                    "market_potential": opp.get('market_potential', 5)
                }
                nodes.append(node)
            
            # è½¬æ¢å…³ç³»ä¸ºè¾¹
            edges = []
            for rel in relationships:
                edge = {
                    "source": rel.get('entity1', ''),
                    "target": rel.get('entity2', ''),
                    "type": rel.get('type', 'related'),
                    "strength": rel.get('strength', 5),
                    "description": rel.get('description', ''),
                    "confidence": rel.get('confidence', 0.5)
                }
                edges.append(edge)
            
            # è®¡ç®—ä¸­å¿ƒæ¦‚å¿µ
            entity_importance = {node['name']: node['importance'] for node in nodes}
            central_concepts = sorted(entity_importance.items(), key=lambda x: x[1], reverse=True)[:10]
            central_concepts = [concept[0] for concept in central_concepts]
            
            # è®¡ç®—è¿æ¥å¼ºåº¦
            connection_strength = {}
            for edge in edges:
                key = f"{edge['source']}-{edge['target']}"
                connection_strength[key] = edge.get('strength', 5)
            
            enhanced_kg = KnowledgeGraph(
                graph_id=f"enhanced_kg_{session_id}",
                nodes=nodes,
                edges=edges,
                central_concepts=central_concepts,
                connection_strength=connection_strength
            )
            
            return enhanced_kg
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºçŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {e}")
            # è¿”å›åŸºç¡€çŸ¥è¯†å›¾è°±
            return KnowledgeGraph(
                graph_id=f"basic_kg_{session_id}",
                nodes=[],
                edges=[],
                central_concepts=[],
                connection_strength={}
            )

    async def _deduplicate_entities(self, entities: List[Dict[str, Any]], 
                                  session_id: str) -> List[Dict[str, Any]]:
        """å»é‡å’Œèšåˆç›¸ä¼¼å®ä½“"""
        try:
            if not entities:
                return []
            
            # ç®€å•çš„åŸºäºåç§°ç›¸ä¼¼åº¦çš„å»é‡
            unique_entities = []
            seen_names = set()
            
            for entity in entities:
                name = entity.get('name', '').lower().strip()
                if name and name not in seen_names:
                    seen_names.add(name)
                    unique_entities.append(entity)
            
            return unique_entities[:50]  # é™åˆ¶å®ä½“æ•°é‡
            
        except Exception as e:
            self.logger.error(f"å®ä½“å»é‡å¤±è´¥: {e}")
            return entities[:50]

    async def _generate_keyword_driven_report(self, keywords_result: Dict[str, Any], 
                                            literature: List[LiteratureDocument], 
                                            knowledge_graph: KnowledgeGraph) -> Dict[str, Any]:
        """ç”Ÿæˆå…³é”®è¯é©±åŠ¨è°ƒç ”æŠ¥å‘Š"""
        return {
            "report_type": "keyword_driven",
            "executive_summary": f"åŸºäºå…³é”®è¯é©±åŠ¨æ–¹æ³•ï¼Œæ£€ç´¢åˆ°{len(literature)}ç¯‡é«˜è´¨é‡æ–‡çŒ®",
            "methodology": "å…³é”®è¯é©±åŠ¨æ–‡çŒ®æ£€ç´¢ç­–ç•¥",
            "key_findings": [
                f"è¯†åˆ«äº†{len(keywords_result.get('core_keywords', []))}ä¸ªæ ¸å¿ƒå…³é”®è¯",
                f"æ„å»ºäº†åŒ…å«{len(knowledge_graph.nodes)}ä¸ªèŠ‚ç‚¹çš„çŸ¥è¯†å›¾è°±",
                f"å¹³å‡æ–‡çŒ®è´¨é‡åˆ†æ•°: {np.mean([doc.quality_score for doc in literature]):.2f}"
            ],
            "literature_overview": {
                "total_papers": len(literature),
                "year_distribution": self._analyze_year_distribution(literature),
                "journal_distribution": self._analyze_journal_distribution(literature)
            },
            "recommendations": [
                "åŸºäºå…³é”®è¯é©±åŠ¨æ–¹æ³•çš„æ·±å…¥ç ”ç©¶å»ºè®®",
                "ç¡®å®šçš„é«˜ä»·å€¼ç ”ç©¶æ–¹å‘",
                "å»ºè®®çš„åç»­è°ƒç ”é‡ç‚¹"
            ]
        }

    async def _generate_topic_modeling_report(self, validated_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆä¸»é¢˜å»ºæ¨¡è°ƒç ”æŠ¥å‘Š"""
        return {
            "report_type": "topic_modeling",
            "executive_summary": "åŸºäºä¸»é¢˜å»ºæ¨¡æ–¹æ³•çš„æ™ºèƒ½æ–‡çŒ®åˆ†ææŠ¥å‘Š",
            "methodology": "ä¸»é¢˜å»ºæ¨¡æ™ºèƒ½å‘ç°ç­–ç•¥",
            "discovered_insights": [
                f"å‘ç°äº†{len(validated_results.get('validated_topics', []))}ä¸ªä¸»è¦ç ”ç©¶ä¸»é¢˜",
                f"è¯†åˆ«äº†{len(validated_results.get('validated_clusters', []))}ä¸ªæ–‡çŒ®èšç±»",
                "æ­ç¤ºäº†éšå«çš„è·¨å­¦ç§‘ç ”ç©¶è”ç³»"
            ],
            "innovation_opportunities": [
                "æ–°å…´ç ”ç©¶ä¸»é¢˜å’Œå‘å±•è¶‹åŠ¿",
                "è·¨å­¦ç§‘ç ”ç©¶æœºä¼š",
                "ç ”ç©¶ç©ºç™½å’Œåˆ›æ–°ç‚¹"
            ],
            "recommendations": [
                "åŸºäºä¸»é¢˜å‘ç°çš„ç ”ç©¶å»ºè®®",
                "æ–°å…´é¢†åŸŸçš„æ¢ç´¢æ–¹å‘",
                "è·¨å­¦ç§‘åˆä½œæœºä¼š"
            ]
        }

    async def _generate_hybrid_research_report(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ··åˆè°ƒç ”æŠ¥å‘Š"""
        return {
            "report_type": "hybrid",
            "executive_summary": "ç»“åˆå…³é”®è¯é©±åŠ¨å’Œä¸»é¢˜å»ºæ¨¡æ–¹æ³•çš„ç»¼åˆæ–‡çŒ®è°ƒç ”æŠ¥å‘Š",
            "methodology": "æ··åˆæ™ºèƒ½è°ƒç ”ç­–ç•¥",
            "comprehensive_findings": [
                "æ•´åˆäº†ä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ¿",
                "æä¾›äº†æœ€å…¨é¢çš„æ–‡çŒ®è¦†ç›–",
                "é€šè¿‡äº¤å‰éªŒè¯ç¡®ä¿äº†ç»“æœå¯é æ€§"
            ],
            "quality_metrics": results.get("comprehensive_quality", {}),
            "strategic_recommendations": [
                "åŸºäºç»¼åˆåˆ†æçš„æˆ˜ç•¥å»ºè®®",
                "å¹³è¡¡ç²¾ç¡®æ€§å’Œå‘ç°æ€§çš„ç ”ç©¶æ–¹æ³•",
                "å¤šç»´åº¦çš„ç ”ç©¶è·¯çº¿å›¾"
            ]
        }

    def _analyze_year_distribution(self, literature: List[LiteratureDocument]) -> Dict[str, int]:
        """åˆ†æå¹´ä»½åˆ†å¸ƒ"""
        year_count = {}
        for doc in literature:
            year = doc.year
            year_count[year] = year_count.get(year, 0) + 1
        return year_count

    def _analyze_journal_distribution(self, literature: List[LiteratureDocument]) -> Dict[str, int]:
        """åˆ†ææœŸåˆŠåˆ†å¸ƒ"""
        journal_count = {}
        for doc in literature:
            journal = doc.journal
            journal_count[journal] = journal_count.get(journal, 0) + 1
        return journal_count

    async def _publish_error_event(self, original_event: BlackboardEvent, error_msg: str):
        """å‘å¸ƒé”™è¯¯äº‹ä»¶"""
        await self.publish_result(
            EventType.CONFLICT_WARNING,
            {
                "error_type": "information_agent_error",
                "original_event_id": original_event.event_id,
                "error_message": error_msg,
                "agent": self.config.name
            }
        )
