收到，我将为你编写一份具备高可执行性的Cursor开发指导方案，涵盖批判Agent与报告生成Agent两大模块。

该方案将包括：

* 每个Agent的职责定位与功能细化
* 输入输出结构说明（包括与黑板的交互字段）
* 基于DeepSeek-R1 API的Prompt模板设计
* Python模块与函数结构建议，适配Cursor开发
* 报告生成Agent的Markdown/HTML结构、引用标记、图表指令生成方法
* 两Agent之间如何通过黑板协同工作、处理批评反馈与引导重写

完成后我会尽快通知你查阅。


# 批判Agent与报告生成Agent实现方案（面向Cursor开发）

本方案针对科研多智能体系统（multi-agent system）中的**批判Agent**和**报告生成Agent**，提供详细的实现思路。内容涵盖两类Agent的职责定位、在任务链中的作用时序、与黑板系统交互的数据结构定义、高质量Prompt模板示例（使用DeepSeek-R1 API进行LLM调用）、关键类与函数设计（模块划分），以及报告生成Agent在结构化输出（Markdown/HTML格式）、引用插入、图表占位等方面的实现要点。设计强调高可行性和逻辑完整性，便于Cursor开发者直接据此编写 `.py` 模块代码。

## 批判Agent 实现方案

### 职责与功能目标

批判Agent的核心职责是在科研任务过程中充当**严谨的审稿人/评论者**，对其他Agent提出的方案和中间结论进行审辩和评估，确保推理过程的逻辑严密、结论合理且符合科研伦理规范。具体功能目标包括：

* **逻辑审查**：检查方案草案中的推理逻辑是否严谨，有无自相矛盾之处或逻辑谬误。识别过度推断、因果倒置等问题。
* **知识与证据检视**：关注方案中是否存在知识盲区或缺少文献支持的论断，确保每项关键论据都有依据。对于未经验证的信息，批判Agent会质疑其可靠性。
* **伦理合规审视**：审查方案是否违反科研伦理或安全合规要求（如涉及不当实验、安全隐患等），这一功能有助于过滤危险或不道德的提议。
* **建设性反馈**：以建设性方式提出改进建议。例如指出具体薄弱环节并建议调用相关Agent获取更多数据或补充实验验证。批判Agent的评论应客观中立、措辞礼貌，但直截了当，帮助团队提升方案质量。
* **质量评分（可选）**：可以对方案按预定维度打分，如**创新性**、**可行性**等，为后续方案优选提供依据。这些评分也是批判Agent反馈的一部分，有助于系统对不同方案进行排序。

通过以上职责，批判Agent在系统中引入一种自我审阅循环，相当于人类头脑风暴后的**同行评议 (peer review)**，可明显提高创意方案的质量与可靠性。

### 在科研任务链中的作用与时序

在多Agent协作流程中，批判Agent通常在**关键阶段或任务完成后**介入，对阶段性成果进行评审。其作用时序如下：

* **并行监控**：批判Agent持续监视黑板上发布的主要中间结果和方案草稿事件。当主Agent或子Agent在黑板上写入“方案草案”或重要结论时，批判Agent会捕捉到相应事件。这通常发生在各子任务基本完成、主Agent整合出初步方案之后。批判Agent也可在过程中多次介入（例如每轮迭代都评审当前进展），实现**异步并行**的审查，不一定要等待所有任务结束才启动。
* **触发条件**：典型地，当**方案初稿生成**（事件如 `solution_draft_created`）时，批判Agent被调度执行一次全面审查。此外，如果黑板出现**冲突警告**（例如验证Agent发现两个结论矛盾）事件，也会优先触发批判Agent分析冲突原因。批判Agent在这些情况下作为**后验验证**角色介入，确保明显问题在最终报告前被发现并处理。
* **顺序与并发**：批判Agent可以与其他Agent并发运行。在实现上，黑板发布事件后，调度器可并行分发任务给批判Agent，使其不阻塞其他Agent工作。由于批判Agent主要依赖LLM推理，其运行可独立进行。不过，为了利用批判结果改进方案，通常主流程会**等待批判反馈**后再进入报告生成等后续步骤。也就是说，批判Agent一般在**验证阶段**之后、报告生成之前完成其工作。
* **多次迭代**：对于复杂课题，批判Agent可能多轮参与。例如第一次审查提出若干改进建议，主Agent根据建议召集相关Agent补充研究，更新方案草案；当新的方案产生时，批判Agent再次审阅确认问题已解决。这种循环直至方案无重大瑕疵或达到预定质量门槛。

**时序总结**：批判Agent贯穿任务执行的中后期，通常**在方案整合阶段触发**，可并行运行但最终在报告生成前给出结论性的批判报告，确保输出给用户的方案经过严格把关。

### 黑板交互的输入输出及JSON结构

批判Agent通过黑板机制与其他Agent解耦通信。它订阅感兴趣的事件（如方案草案生成事件），从黑板读取输入数据，并将审查结果以结构化JSON写回黑板。下面详细列出批判Agent的输入、输出字段及数据格式约定：

* **订阅事件**：`"solution_draft_created"`（方案草案生成）事件，或类似的黑板标记。当黑板上出现该事件（附带方案草案内容），调度器通知批判Agent处理。批判Agent也可订阅`"conflict_warning"`（冲突警告）事件，以便在侦测到知识矛盾时介入。

* **输入数据**：批判Agent从黑板读取一个或多个字段作为输入：

  * `draft_id`：草案方案的唯一标识符，用于引用该草案。
  * `draft_content`：草案方案的具体内容文本（可能是答案草稿、研究方案等）。这是批判Agent评审的主要对象。
  * `context`：（可选）任务的上下文信息，如原始科研问题、任务要求摘要，方便批判Agent理解方案背景。
  * 其他元数据：诸如草案作者Agent标识、时间戳等，可用于记录但非批判逻辑必需。

  *示例*: 当主Agent在黑板发布方案草案时，黑板事件内容可能是：

  ```json
  {
    "event": "solution_draft_created",
    "data": {
      "draft_id": "draft_123",
      "draft_content": " ...（方案草稿具体内容）...",
      "context": " ...（问题描述或任务目标）..."
    }
  }
  ```

  批判Agent监听到该事件后，会提取其中的 `draft_content` 进行分析。

* **输出数据**：批判Agent完成审查后，将结果写入黑板，通常发布一个\*\*“批判反馈”事件\*\*（如 `critique_feedback`）。输出内容采用JSON结构，便于机器和人类均可读取：

  * `agent`: 标识来源Agent，例如 `"CritiqueAgent"`（批判Agent）。
  * `event`: 事件类型，如 `"critique_feedback"` 或 `"verification_report"`（验证报告）。后者可用作批判Agent输出的通用命名，表示这是一份审查/验证后的报告。
  * `data`: 一个包含具体反馈内容的对象，包括：

    * `target_draft_id`: 所批评草案的ID（如 `"draft_123"`），便于关联该反馈与对应方案。
    * `critique`: 批判意见的全文，列举发现的问题、逻辑漏洞、不足之处等。文本应结构清晰，可带序号罗列多个问题。
    * `recommendation`: 改进建议的文本。如果有多条建议，可在文本中用项目符号列出。
    * `score`: （可选）对方案的评分或评价，用数值或等级表示。可按需提供多个维度的评分，例如 `{ "feasibility": 7, "novelty": 8 }`。
    * `timestamp`: 时间戳或序列号，用于记录反馈生成时间。

  *输出示例*: 批判Agent发现方案存在高电压造成材料不稳定的问题，给出反馈：

  ```json
  {
    "agent": "CritiqueAgent",
    "event": "critique_feedback",
    "data": {
      "target_draft_id": "draft_123",
      "critique": "方案中对高电压操作的假设缺乏依据，可能导致电解液分解:contentReference[oaicite:12]{index=12}。应考虑材料稳定性或降低工作电压。",
      "recommendation": "建议引入实验设计Agent验证高电压条件下材料的稳定性，并查阅相关文献支持该假设。",
      "score": { "逻辑严谨度": 6, "安全性": 5 }
    }
  }
  ```

  黑板上其它Agent（例如主Agent）订阅了`critique_feedback`或查看黑板内容，便可以读取该JSON并采取相应措施（如调整方案或记录评分）。

* **黑板存储**：每条黑板记录应包含通用字段（如记录ID、来源Agent、类型、内容等）。批判Agent输出以 JSON 形式存储，确保格式一致利于解析。如采用文本形式发布，则需遵循约定格式（例如内容前加“\[批判]”标签）以明确身份，但推荐使用JSON以利于后续Agent程序解析。

综上，批判Agent通过黑板输入方案草案，输出结构化的批判报告/验证报告 JSON，作为后续Agent（包括主Agent和报告生成Agent）的参考依据。

### DeepSeek-R1 Prompt模板（中文）

为有效利用LLM（通过DeepSeek-R1 API）实现批判Agent的审辩功能，需要精心设计Prompt模板。该模板以中文撰写，明确批判Agent的角色和任务要求，使大模型输出所需格式的批评意见。基于设计要求，Prompt模板要点如下：

* **角色设定**：强调批判Agent是“严谨的科研审稿人/评审者”，专业、客观、公正。让模型以审稿人的视角工作。
* **任务描述**：明确要求模型**找出方案中的问题并指出**，涵盖逻辑漏洞、证据不足、伦理问题等。要求输出具体、中肯的批评，以及改进建议。
* **输出格式**：可以指示模型以条理清晰的方式输出，如先列出发现的问题（编号列表），然后给出总体建议。也可以要求模型使用特定标记（如“不足：... 建议：...”）来区分批评点和建议。风格应专业礼貌，不使用过激言辞，保持建设性。
* **Few-shot示例**：为提高稳定性，可在系统提示中提供1-2个示例，让模型参考回答风格。例如一个示例批评某方案逻辑不足并提出建议的对话。

**批判Agent Prompt模板示例**（中文，适用于DeepSeek-R1 API 调用）：

```text
系统角色提示: 
你是一位严谨的科研审稿人，擅长从科研方案中发现问题并提出改进建议。现在你需要审阅以下研究方案草稿，从逻辑性、论据充分性和伦理规范等方面进行批判性评估。请找出方案中的不足之处，每一条指出具体问题并解释原因，然后给出相应的改进建议。

评审要求: 
1. 若发现推理链不严谨或有逻辑谬误，指出在哪一步出现问题，以及可能的正确做法。  
2. 若发现论据/数据不足，指出需要补充哪些证据或参考文献。  
3. 若发现方案有伦理或安全隐患，明确指出违规之处，并建议如何避免。  
4. 语气专业客观，直接指出问题，措辞礼貌不过度修饰。  
5. 格式: 以**“问题1:”、“问题2:”**分条列出发现的问题，每个问题后附**“建议:”**给出改进建议。

现在开始审阅方案草案。
```

```text
<研究方案草案内容插入位置>
```

上述Prompt的**系统提示**部分预先设定批判Agent身份和任务要求，接着将在用户输入部分插入黑板上的方案草案 (`draft_content`)。DeepSeek-R1 接收到此完整提示后，将生成批判意见和建议。

*实现提示*: 在实际代码中，可使用一个多行字符串模板，将`draft_content`通过字符串替换或格式化插入。例如：

```python
prompt_template = """你是一位严谨的科研审稿人...现在开始审阅方案草案。\n\n方案草案:\n{draft}\n\n请给出评审："""
prompt = prompt_template.format(draft=draft_content)
response = llm_client.generate(prompt)  # 通过DeepSeek-R1 API获取LLM输出
```

DeepSeek-R1 API 与 OpenAI接口兼容，可直接传入prompt获取模型输出。通过统一的 `llm_client.generate()` 接口调用，便于后续更换模型或集成上下文缓存等优化。模型返回的文本再由Agent封装进黑板输出JSON。

**Few-shot**：如果需要，可在 `prompt_template` 的系统部分加入示例，例如：“示例：方案假设A无依据——问题：...；建议：...”。这将进一步引导模型输出遵循预期格式。

### 类与函数设计及模块划分

在Cursor开发环境下，我们将批判Agent实现为独立的Python模块和类，遵循面向对象设计，方便后续维护与扩展。以下是关键类、函数的命名和职责说明：

* **模块文件**：`agents/critique_agent.py`（假定项目按模块组织）

* **类名**：`CritiqueAgent`，继承自通用Agent基类（如有）。如果系统已有 `BaseAgent` 抽象类（例如定义了Agent名称、订阅事件列表、处理流程等），则CritiqueAgent应继承并实现其中的抽象方法。

* **关键属性**：

  * `name`: 标识Agent名称，如 `"CritiqueAgent"`，用于日志和黑板记录。
  * `subscriptions`: 订阅的事件类型列表，如 `["solution_draft_created", "conflict_warning"]`。调度器据此将对应事件分派给该Agent处理。
  * （可选）`llm_client`: LLM调用接口实例，用于发送Prompt并获取回复。所有LLM调用通过统一的DeepSeek-R1 API封装，例如一个 `DeepSeekClient` 类实例。

* **关键方法**：

  * `process(event)`: 批判Agent处理入口，由调度器调用或事件驱动触发。参数`event`包含黑板事件数据（例如方案草案）。`process`方法执行以下步骤:

    1. 从`event`提取 `draft_content` 等必要信息。
    2. 调用`generate_critique(draft_content)`获取批判反馈文本。
    3. 将反馈文本解析/包装成黑板输出数据（填充JSON结构），调用黑板接口写入。
    4. 结束处理，记录日志。
  * `generate_critique(draft:str) -> dict`: 核心功能函数，生成对给定方案草案的批评建议。内部会构造Prompt（利用上述模板，将`draft`插入），然后通过`self.llm_client.generate(prompt)`调用大模型。得到模型输出后，可进一步格式化为需要的结构，例如拆分问题列表和建议。最终返回包含`critique`和`recommendation`等字段的字典，可供`process`封装进JSON。
  * `publish_feedback(data: dict)`: 将批判结果发布到黑板的辅助函数。封装黑板交互逻辑，例如调用黑板模块的API `blackboard.publish(event_type, data)`。批判Agent可能直接调用此方法，也可能通过基类提供的统一发布接口。

* **模块划分与依赖**：

  * `base_agent.py`: 定义Agent基类 `BaseAgent`，其中声明 `subscriptions` 和抽象方法 `process(event)` 等。也包含通用的 `subscribe()`、`publish()` 方法，用于与黑板交互。CritiqueAgent作为子类实现`process`。
  * `llm_client.py`: 定义 DeepSeek-R1 API 的封装类，例如 `DeepSeekClient`，内部通过HTTP或SDK调用模型接口。提供`generate(prompt)`方法返回文本。各Agent通过组合该client对象来使用LLM能力。
  * `blackboard.py`: 定义黑板交互模块，提供线程安全的发布/订阅功能。如使用Redis Streams/Celery，可在此模块封装Redis发布订阅或Celery队列的读写操作。CritiqueAgent通过调用 `blackboard.publish()`或由调度框架自动处理事件分发。

* **与AutoGen/LangGraph**：若选择使用AutoGen、LangGraph等框架，可简化Agent对话逻辑和任务图设计。例如AutoGen可用于搭建一个User/Assistant模式的对话Agent，但对于批判Agent这种**单次输入->输出**的任务，直接使用自定义类更透明可控。LangGraph可用于设计任务流程图，将批判Agent节点接入黑板事件流。但本方案不强依赖这些框架，仅在模块设计时保持开放性，确保容易集成它们（例如BaseAgent接口设计与AutoGen Agent类兼容）。

通过上述类与函数划分，批判Agent模块职责单一、接口清晰。在Cursor中开发者可以直接按照此结构创建 `critique_agent.py`，实现对应的类和函数，并在系统初始化时注册该Agent使其开始订阅黑板事件。

### 黑板交互典型流程示例

以下演示批判Agent在黑板交互中的一次典型调用流程，展示其如何与其他组件协同工作：

1. **方案草案发布**：主Agent整合各子任务结果，在黑板写入方案草案，例如事件类型`solution_draft_created`，附带草案内容和ID（如前述JSON示例）。黑板通过发布/订阅机制通知所有对此事件感兴趣的Agent。CritiqueAgent订阅了该事件，因此收到通知。
2. **批判Agent执行**：调度器（或黑板自身）调用 CritiqueAgent 的 `process(event)` 方法，将方案草案内容传递给它处理。CritiqueAgent提取草案文本，调用LLM生成批评反馈：

   * 调用 `generate_critique(draft_content)` 构造 Prompt 并请求DeepSeek-R1模型，得到审查结果文本。
   * 将结果文本整理成批判反馈数据结构（问题列表及建议等）。
3. **反馈发布**：CritiqueAgent 调用 `publish_feedback(data)`，在黑板记录一条新事件，例如：

   ```json
   { "agent": "CritiqueAgent", "event": "critique_feedback", "data": { ... } }
   ```

   这条记录被写入黑板存储，并通过Pub/Sub分发给订阅了`critique_feedback`或通用通知的组件。
4. **主Agent接收反馈**：主Agent监视黑板上批判反馈事件，读取CritiqueAgent的输出。例如发现批判指出方案中有高电压稳定性问题，需要补充实验验证。主Agent据此决定追加一个子任务：创建“实验设计Agent”去设计验证实验，或让“文献Agent”检索相关文献支持。这一新的任务由主Agent发布到黑板，触发相应Agent执行。
5. **方案修正迭代**：假如相关Agent执行后，在黑板更新了方案内容（例如降低电压条件并附实验分析），主Agent形成新的方案草案。批判Agent可能再次被触发审查新草案。在该示例中，CritiqueAgent第二次审查后认为方案合理，无重大批评，可能给出较高评分并反馈“方案考虑了电解液稳定性，创新度高，可行性良好”。
6. **流程继续**：批判Agent的反馈最终将与方案一起提供给报告生成Agent，用于最终报告的撰写或说明依据（例如报告中附带“该方案已通过内部评审，无明显漏洞”之类的表述）。

上述流程说明了批判Agent在黑板驱动的架构中如何协同运作：**事件触发 -> LLM分析 -> 反馈发布 -> 影响后续任务**。通过这种循环，系统逐步完善方案，确保输出给用户的方案经过多轮把关和优化。

## 报告生成Agent 实现方案

### 职责与功能目标

报告生成Agent的主要职责是将最终的研究方案和支撑论证**整理成一份结构化的报告**，以方便人类阅读和理解。它扮演**科技写作助手**的角色，在输出阶段关注格式和表达。具体功能目标：

* **分节组织内容**：按照科研报告或论文的典型结构，对内容进行分段分类，包括摘要、研究背景、方法与过程、实验设计、结果与讨论、结论等部分。这样可确保报告层次清晰，逻辑严谨。报告生成Agent需将杂乱的中间结果整理成有序的章节。
* **凝练与表述**：将技术内容用精确、简洁的语言表述，保证专业性和可读性。它对其他Agent提供的内容进行**润色和整合**，去除冗余、纠正语句不通顺之处，使整篇报告连贯流畅。特别地，确保术语使用一致，说明清楚。
* **整合证据与引用**：把批判Agent、文献Agent等提供的**证据、引用资料**融入报告，在相关句子处添加引用标注，例如【1】、【2】等，并在报告末尾列出参考文献清单。这增强了报告的可信度和学术风格。
* **图表插入**: 支持在报告中插入图表或示意图的**占位符**。当有需要解释的复杂数据或结构时，报告生成Agent可以插入如“图1”“表1”等占位符，并配上简短的标题/说明，提示后续由可视化Agent或人工添加实际图表。这确保报告格式的完整性。
* **最终输出格式**：生成符合Markdown或HTML等结构化格式的文档，使报告既可用于前端展示，又可供后续Agent读取处理。Markdown可以方便地转换为HTML等格式，兼具可读性和可解析性。报告生成Agent需严格按照指定格式输出（如使用Markdown的标题、列表、引用语法等），这是其重要目标之一。

总之，报告生成Agent聚焦**结构化输出与表达完善**。它不再引入新的事实推理，而是对现有内容进行组织加工，产生最终交付成果。这份报告既是给用户的最终答复，也是系统知识沉淀的一部分，可能被存储以供将来查询或被其他Agent利用。

### 在科研任务链中的作用与时序

报告生成Agent通常位于科研任务流程的**终端阶段**，在所有内容准备就绪后负责收尾整合。其作用时序如下：

* **触发时机**: 当主要研究任务的子环节都完成，并且主Agent已选择最终方案或形成全面结论时，触发报告生成Agent开始工作。具体来说，系统检测到**方案草案完成**且**批判/验证反馈已提供**时，即黑板上存在`draft_answer`（最终方案草稿）以及对应的评审报告（如`critique_feedback`或`verification_report`），则意味着可以产出最终报告。此时通过调度策略启动报告生成Agent。
* **顺序执行**: 报告生成Agent一般**在所有其他Agent之后**顺序执行，避免抢先生成报告而内容不完整。在批判Agent等验证Agent运行完毕后，报告生成Agent才开始，以确保报告内容反映最新的改进和审查结果。因此它通常不是并发执行，而是**串行的最终一步**。
* **与主Agent协同**: 有两种触发方式：1）主Agent主动调用报告Agent，将整理任务交给它（例如通过函数或黑板任务发布方式）；2）黑板事件驱动，报告Agent订阅特定事件（如 `ready_for_report`），在条件满足时自动运行。无论哪种方式，报告生成Agent承担**收尾汇总**的职责，其运行标志着一次任务流程即将结束。
* **耗时与异步**: 报告生成可能涉及较长的文本生成和格式调整，因此可以作为异步任务执行（例如在Celery中作为最后一个任务）。虽然它在逻辑上是最后阶段，但仍可在独立Worker中执行，不阻塞主调度。另外，为保持响应流畅，若报告很长，可考虑逐步生成（例如先生成提纲，经主Agent确认后再充实内容），但一般一次性调用LLM生成完整报告更简便。

**时序总结**：报告生成Agent在**所有研究和验证工作完成后**启动，作用是**将最终成果编撰成文**。它标志任务流程的收尾，通过整合黑板上的**问题、方案、批判反馈、引用**等信息，输出最后的报告文档。这一报告既是给用户的答案，也是系统内部知识库的记录，可被后续类似任务复用或被评价Agent进一步审阅。

### 黑板交互的输入输出及JSON结构

报告生成Agent同样通过黑板读取所需信息和输出生成的报告。由于报告需要综合多个来源的数据，其输入比批判Agent更丰富，输出则是最终文档形式的内容。下面是其黑板交互数据设计：

* **输入数据**：报告Agent需要从黑板收集各类信息，以确保报告完整准确：

  * `final_plan` / `draft_answer`：**最终方案内容**。这通常由主Agent或Aggregation模块产出，包含对问题的解答或创意方案的完整描述。
  * `analysis_summary`：**研究摘要/背景**。可能由主Agent在规划阶段写入黑板，用于记录对问题的理解、背景信息。如果没有明确提供，报告Agent可从其它内容中提炼背景。
  * `method_details`：**研究方法或过程**。来自各子Agent输出（如实验设计Agent的方案、模型Agent的计算过程）。黑板上可能有结构化的实验方案草案、模型计算结果说明等。报告Agent应提取其中关键内容放入方法/过程章节。
  * `critique_feedback`：**批判反馈**。来自批判Agent的评审结果（逻辑问题及改进）。报告Agent需要检查方案是否已根据反馈改进，并决定如何在报告中呈现这些信息——可能在讨论部分提及方案经过内部评审，或者将未解决的问题作为局限写入讨论。
  * `references`：**参考资料列表**。可能由文献Agent或CitationAgent提前查询并存入黑板的文献信息。这可以是一个列表结构，每项包含来源标题、作者、链接等。报告Agent在正文引用相应编号，在参考文献部分罗列详细信息。若没有现成列表，报告Agent需从各段内容的`source`字段汇总出处，或者调用辅助Agent生成引用列表（例如引用Agent，见后述）。
  * 其他：**图表信息**（如有）。如果可视化Agent生成了图表或分析结果，黑板上可能有图片路径或描述。报告Agent可据此插入图表占位符和说明文字。若无图表数据，Agent也可根据需要自主决定插入占位符，让后续人工补充。

  *数据获取方式*：报告Agent可以通过黑板模块提供的接口查询所需内容。例如：

  ```python
  plan = blackboard.get_latest("final_plan")
  summary = blackboard.get("analysis_summary", task_id)
  feedback = blackboard.get("critique_feedback", target_id=plan.id)
  refs = blackboard.get("reference_list")
  ```

  其中`get`方法支持根据类型或关联ID获取黑板记录。由于各数据格式不尽相同，报告Agent需要一定的逻辑判断可用性，如果缺失某部分信息，可尝试通过LLM推断补全（例如没有明确背景时，从问题描述生成一段背景介绍）。

* **输出数据**：报告生成Agent的输出是在黑板上发布最终报告内容，一般使用事件类型`"final_report_generated"`，并携带报告文本。典型JSON结构：

  * `agent`: `"ReportAgent"`（报告生成Agent）。
  * `event`: `"final_report_generated"` 或类似标识最终报告的事件。
  * `data`: 包含报告具体内容的对象：

    * `report_content`: 最终报告的正文，支持Markdown或HTML格式的多段文本。由于JSON嵌入大块文本不便，也可以将report作为黑板中一个富文本字段存储。
    * `format`: 报告格式标识，如 `"markdown"` 或 `"html"`，以告知渲染方式。
    * `metadata`: （可选）如报告生成时间、涉及任务ID、版本等元数据。

  *输出示例*（Markdown格式报告片段，JSON的`report_content`字段内容示意）：

  ```markdown
  # 摘要  
  本研究提出了一种新的催化剂设计方案...（摘要内容）...  

  ## 背景  
  催化剂在化学反应中起到...（背景内容，包括引用如【1】）...  

  ## 方法  
  本方案采用了密度泛函理论(DFT)计算来筛选材料，并由实验设计Agent制定实验流程验证关键假设...（方法内容）...  

  ## 结果与讨论  
  DFT计算结果表明材料A的带隙为...，满足反应要求。实验方案预期验证高电压对电解液稳定性的影响。批判Agent审阅指出需要关注高电压下电解液分解的风险，我们已在方案中引入相应控制措施...（讨论内容，包括对批判反馈的响应）...  

  *（此处插入图表占位符，如图1）*  

  ## 结论  
  综合计算和初步实验设计，本方案在能量密度和稳定性上具有可行性...（结论内容）...  

  ## 参考文献  
  1. 张三等人，<u>“高能量密度电池材料综述”</u>，Journal of Battery Research, 2022.  
  2. ... （更多文献条目）  
  ```

  在JSON中，`report_content`会是包含上述Markdown文本的长字符串。黑板记录可能不方便直接存储如此长文本，可考虑存储在数据库中，黑板里只保留引用或简要摘要。然而作为最终结果，也可以直接将Markdown文本放入黑板，供前端读取显示。

* **黑板后续用途**：最终报告既提供给用户，也可能被系统存档至知识库（PostgreSQL 保存）。后续若有相似任务，主Agent或其他Agent可以检索相关报告以复用信息或进行持续学习。因此，这里输出的结构化报告本身也是黑板中的一类知识片段，可被**学习Agent**或**评估Agent**进一步处理，实现知识累积和质量评估。

### DeepSeek-R1 Prompt模板（中文）

为了让LLM胜任报告撰写，需要设计一个复杂的Prompt模板，引导模型将提供的内容要素组装成格式良好的报告。该Prompt需要详细指示报告结构、包含的要点和格式规范。要点如下：

* **角色设定**：将模型定位为*科研报告撰写助手*，具有出色的学术写作能力。提示它按照学术报告标准格式进行工作，语言要求正式客观。
* **输入内容说明**：清晰列出模型可用的信息来源，包括最终方案、背景材料、方法细节、批判意见、引用列表等。可以在Prompt中以占位符标明这些部分，并提供实际内容供模型引用。模型需要明白这些是写作素材，应在报告中适当引用和整合。
* **报告结构要求**：在Prompt中列出报告的预期章节，如“摘要、背景、方法、实验设计、结果与讨论、结论、参考文献”等。可要求模型按照这些章节输出，并在输出中使用Markdown格式的标题（如`# 摘要`、`## 背景`等）。
* **格式细节**: 提示模型：

  * 使用Markdown语法（或HTML标签）表示标题、段落和列表；
  * 在需要引用的地方以\*\*【数字】\*\*形式标注引用编号，并确保在最后“参考文献”部分按编号列出文献信息；
  * 如果需要图表，使用占位符，例如插入“（见图1）”文本或提供一个Markdown图片标签格式，但使用占位链接。例如：`![图1: 实验流程示意图](placeholder://figure1)`。并在正文相应位置写说明，如“如图1所示，...”。
  * 保持内容衔接和连贯，每节之间自然过渡。避免简单拼接，需必要的解释句将提供的内容串联。
* **长度和详略**：可以指示各部分内容大致篇幅，如摘要尽量压缩重点，背景交代清楚理论和现状，方法详细描述过程，讨论需要分析方案优缺点（包括批判Agent的意见），结论简明扼要等。
* **几段示例**：可选地，给模型一个简短示例提纲或段落，示范如何引用文献和插入图表。例如：
  `"示例：在背景中引用相关工作【1】，在结果中指出“如图1所示”并在稍后插入图1占位符。参考文献部分格式如下...（提供一个参考文献列表示例）"`
  这能帮助模型输出符合格式的内容。

**报告生成Agent Prompt模板示例**（中文）：

```text
系统角色提示: 
你是一名智能科研助理，负责将科研团队的最终方案整理为正式报告。报告需结构清晰、语言专业、包含必要的引用和图表。请根据提供的素材撰写完整报告，格式要求：使用Markdown分节，包括“摘要、背景、方法、实验设计、结果与讨论、结论、参考文献”等章节。正文中需要在合适位置插入引用【*】标记，并在参考文献列表中给出详细出处；如有需要解释的图表，用“图n”占位并配说明。

请综合以下提供的信息完成写作：

1. **研究问题与方案摘要**：{summary_info}  
2. **背景材料**（含重要文献/理论）：{background_info}  
3. **方法与实验设计**：{method_info}  
4. **最终方案详细**：{plan_content}  
5. **内部评审意见**：{critique_points}  (批判Agent的主要反馈，供讨论部分参考)  
6. **参考文献列表**：{reference_list}

写作要求：中文输出；采用第三人称学术写作风格；内容衔接自然，不逐条罗列素材而是适当整合。突出方案创新点和支撑依据，对存在的不足或假设也予以说明（可参考内部评审意见）。确保所有引用采用【数字】格式标注，并在最后列出对应的参考文献条目。

现在请开始撰写报告：
```

```text
<在此插入各部分内容：summary_info、background_info、...等黑板提取的信息>
```

解释：上述Prompt首先以**系统提示**形式规定了报告Agent的角色和写作要求，然后列出了**6类提供的信息**（以编号列表形式）。这些信息（如 `{plan_content}` 等）将由代码在运行时填充黑板中获取的实际内容。例如：

* `{summary_info}` 替换为主Agent记录的任务摘要或研究目的描述；
* `{background_info}` 替换为文献Agent提供的背景材料要点；
* `{method_info}` 替换为实验设计Agent提供的方法和实验步骤说明；
* `{plan_content}` 替换为最终方案具体描述（可能与摘要类似但更细节）；
* `{critique_points}` 替换为批判Agent给出的主要意见摘要，例如“需要关注XX问题，我们已采取YY措施”等；
* `{reference_list}` 替换为黑板上的参考文献清单（每条文献的引用信息，用作最后章节内容）。

通过在Prompt中直接给出素材要点，模型可以**基于素材进行组织润色**，而非凭空生成，从而保证内容准确并贴合提供的信息。Prompt也明确要求了格式（Markdown章节、引用标记），模型据此输出时会遵循这些规范。

*实现提示*：在代码里，可以先从黑板收集上述各部分内容，然后用`str.format()`或模板引擎将它们插入Prompt字符串：

```python
prompt_template = """你是一名智能科研助理...现在请开始撰写报告：\n\n1. **研究问题与方案摘要**: {summary}\n2. **背景材料**: {background}\n..."""
prompt = prompt_template.format(summary=summary_info, background=background_info, method=method_info, plan=plan_content, critique=critique_points, reference_list=ref_list_text)
report_markdown = llm_client.generate(prompt)
```

DeepSeek-R1 API 返回的 `report_markdown` 就是完整的Markdown文本。然后报告Agent将其包装进黑板输出（见上节）。如果模型输出不符合完全要求，Agent也可执行简单后处理，如检查是否有所有章节，是否包含参考文献段落，否则可以补充或再次调用模型微调Prompt。

### 类与函数设计及模块划分

报告生成Agent的实现同样遵循模块化设计，以便开发者轻松维护。关键设计如下：

* **模块文件**: `agents/report_agent.py`。

* **类名**: `ReportGenerationAgent`（或简称`ReportAgent`）。继承自 `BaseAgent` 基类，与CritiqueAgent类似。

* **属性**:

  * `name`: `"ReportAgent"`。
  * `subscriptions`: 订阅事件列表，例如 `["finalize_solution"]` 或空列表。如果采用主Agent直接触发模式，此列表可为空，由主线程调用Agent启动。如果采用事件驱动，则定义如 `"all_tasks_completed"` 或 `"ready_for_report"` 事件。具体订阅策略视实现而定。
  * `llm_client`: 同样集成 DeepSeek-R1 接口实例，用于生成报告文本。

* **关键方法**:

  * `process(event)`: 报告Agent的处理入口。当触发条件满足时调用。流程:

    1. 调用`assemble_content()`汇集黑板上的各类信息，组织成报告需要的素材（摘要、背景、方法、方案、批判意见、参考文献等文本片段)。
    2. 调用`generate_report(materials)`生成报告文本。其中`materials`可以是一个包含各部分内容的字典，或由Agent内部直接使用属性传递。`generate_report`会构造Prompt并调用LLM生成Markdown格式报告。
    3. 拿到模型输出后，调用`publish_report(report_text)`将结果发布到黑板。
    4. 记录日志，结束。

  * `assemble_content() -> dict`: 从黑板检索报告所需的各项内容。可以细分为多个辅助函数，例如：

    * `_get_summary()` 获取研究摘要（可能从主Agent的记录或根据问题生成简短概述）；
    * `_get_background()` 获取背景材料（如文献综述要点）；
    * `_get_methods()` 获取方法和实验设计细节；
    * `_get_final_plan()` 获取最终方案内容；
    * `_get_critique_points()` 获取批判Agent的主要反馈意见，要以适合融入讨论部分的方式表述；
    * `_get_references()` 获取参考文献列表并格式化为文本。
      这些函数返回的字符串/列表构成材料字典`materials`。如果某部分缺失，函数应返回空字符串或做降级处理（例如没有背景则跳过该段落或用通用语句代替）。

  * `generate_report(materials: dict) -> str`: 根据组装的内容字典，填充Prompt模板并调用LLM生成报告。按照上一节Prompt设计，将materials中的各项插入Prompt中的占位符，然后通过`self.llm_client.generate(prompt)`获取结果。若模型输出是Markdown文本，需要进一步验证格式完整性（例如检查是否包含所有章节标题），必要时可以进行简单的正则检查或多轮生成。

  * `publish_report(report_md: str)`: 将生成的Markdown报告发布到黑板。内部构造黑板记录的数据结构（`event="final_report_generated"`, `data={"report_content": report_md, "format": "markdown"}`），调用`blackboard.publish()`写入。同时可调用数据库接口保存报告全文，以便持久存储。

* **模块划分**:

  * 前述`BaseAgent`, `llm_client`, `blackboard` 模块对报告Agent同样适用，不赘述。
  * 如果系统有**CitationAgent**（引用生成Agent），ReportAgent可以与其配合。例如设计CitationAgent从黑板内容自动搜集引用并生成参考文献列表。本方案未独立实现CitationAgent，而是假定引用列表已由前序步骤提供或通过ReportAgent自身的`_get_references`汇总。因此，可在模块中额外实现一个简单的`ReferenceFormatter`类或函数，负责处理引用格式。

* **与框架集成**:

  * 若使用LangGraph等，可将报告Agent建模为任务图的汇合节点，其输入是多个前置节点（各Agent输出）的集合，然后应用一个转换函数（即调用LLM组合文本）输出报告。
  * 若使用AutoGen，可将各Agent设为消息循环中的参与者。但报告Agent更像是单向文本生成，直接实现更明晰。

通过上述类和函数，报告生成Agent模块职责清晰：从黑板取数据 -> 利用LLM生成报告 -> 输出报告。其中各辅助函数让代码结构匹配报告章节，方便日后调整模板或增加新章节内容。

### 黑板交互典型流程示例

演示报告生成Agent在一次任务中的工作流程：

1. **检测触发条件**：假设主Agent在黑板发布了最终方案草案（`draft_answer`）且批判Agent已提交验证报告（`verification_report`）。调度器判断任务已进入收尾阶段，于是激活报告生成Agent。可以通过黑板事件 `"ready_for_report"`（内含任务ID）通知，ReportAgent订阅此事件并开始处理。
2. **内容汇总**：ReportAgent 调用 `assemble_content()` 从黑板提取信息：

   * 获取任务对应的问题描述和主Agent的方案摘要作为摘要内容；
   * 获取背景文献要点，例如文献Agent曾经写入黑板的“背景知识”条目；
   * 获取方法和实验设计内容，比如实验设计Agent产出的实验方案草稿；
   * 获取最终方案详细内容（可能和摘要相似，但更全面的阐述）；
   * 获取批判Agent反馈的主要结论，以备写入讨论；
   * 获取引用列表：如果黑板上已有`reference_list`条目，则直接使用，否则遍历上述内容找出所有【引用】标记汇总，并在需要时通过查询文献库补全信息。
3. **生成报告**：ReportAgent 调用 `generate_report(materials)`。它根据汇总内容构造Prompt，并通过 DeepSeek-R1 模型生成Markdown格式的报告文本。LLM综合各部分内容，产出连贯的报告。输出例例如上节Markdown示例所示，包含各章节、引用标记和参考文献列表。
4. **发布报告**：ReportAgent 将生成的报告文本用 `publish_report()` 发往黑板：

   ```json
   {
     "agent": "ReportAgent",
     "event": "final_report_generated",
     "data": {
       "report_content": "...(Markdown 文本)...",
       "format": "markdown"
     }
   }
   ```

   主Agent或前端监听到此事件后，获取报告内容，标志任务完成，可以将报告返回给用户查看。黑板中也保存了这份报告，赋予唯一ID并持久化到数据库。
5. **后续处理**：前端UI接收到报告后，渲染Markdown生成最终展示页面（支持折叠章节、点击引用查看文献等增强功能）。如果还有后续Agent（如评估Agent），它可以对这份报告进行分析评分，例如从报告文本评估措辞是否符合要求、检测是否遗漏内容等。另外，如果用户对此有反馈（例如要求修改某部分），系统可将反馈记录到黑板并由相应Agent（可能还是ReportAgent）修改报告后重新生成版本。

通过此流程可见，报告生成Agent将黑板上分散的信息转化为**统一的文字报告**输出。这既完成了对用户的交付，也把多Agent协作的成果固化为知识。整篇报告内容结构良好、证据齐全，体现了最终整合价值。

### 报告输出结构、引用与图表插入

报告生成Agent输出的文档在格式和内容组织上有特殊要求，直接关系到最终结果质量。本节进一步指导如何实现**分节文档输出**、**引用插入**和**图表占位**功能，使输出报告达到高质量标准：

* **多层次章节结构**：利用Markdown的标题语法实现清晰的分节：一级标题（#）用于报告主标题或摘要标题，二级标题（##）用于主要章节，小节可用三级标题（###）等。ReportAgent在Prompt或后处理阶段应确保每个预期章节都有相应标题。例如上文示例中，从摘要到参考文献均有标题标识。这种层次结构便于读者浏览扫描。Cursor开发时，可将章节列表作为常量或配置，方便修改（如某些报告可能不需要“实验设计”章节则可以调整）。

* **内容排版与段落**：控制每段长度适中（3-5句）以增强可读性（避免大段长文压缩在一起）。ReportAgent在整合内容时，可以在素材段落之间加上换行，或者在Prompt提示模型输出时强调“每段不宜过长”。使用Markdown列表格式整理要点时（如需要列举贡献、步骤），Prompt中可示例 `- 要点1` 列表语法，让模型照此输出。

* **引用插入**：学术报告必须给出来源引用以增强可信度。ReportAgent应整合系统内获取的参考资料：

  * 在正文内容中插入引用标记：采用中国高校常用格式\*\*【数字】\*\*（全角方括号）或英文格式\[数字]均可，但需统一。这里使用【1】风格示例。模型输出时，只需在相关句末附上如【1】即可，数字先占位，稍后统一编号。
  * 生成参考文献列表：根据黑板上的`reference_list`或素材内容形成文献列表，在报告末尾列出。每条以序号开头，后接文献信息（可含作者、标题、期刊、年份等）。如果黑板提供的是纯URL或DOI，需要ReportAgent将其格式化成人类可读引用条目（这可通过模板或额外查库实现）。
  * 保证引用编号一致：ReportAgent需要确保正文引用标号和文献列表对应。例如模型可能输出多个引用标记【1】【2】【3】，Agent要匹配这些编号和实际文献条目。如果模型未严格按提供的文献列表输出，可采取策略：**预处理**：在构造Prompt时先将参考文献列表按顺序列出，让模型据此引用对应序号；**后处理**：扫描模型输出的引用标记，如有错乱则调整顺序或补充缺少的条目。
  * *自动引用Agent（可选）*: 在Anthropic的架构示例中，最终报告生成前专门调用CitationAgent添加引用。如果实现复杂引用解析超出ReportAgent职责，可考虑先由CitationAgent完成插入，然后ReportAgent再生成最终报告。但在本方案中，假定ReportAgent自身能够处理简单引用场景。

* **图表占位**：为了报告的完备性，ReportAgent可以插入图、表的占位符：

  * **图像占位**：当需要图示说明时，使用Markdown语法插入。例如：

    ```markdown
    ![图1: 实验装置示意图](assets/fig1_placeholder.png)
    ```

    其中路径指向一个占位图片（比如一张空白或模板图），或者干脆使用一个自定义协议如`placeholder://fig1`标识（渲染层可特殊处理）。同时在正文适当位置写“如图1所示，…”。如果不采用Markdown图片语法，也可简单标记“（图1：XX示意图，占位）”。
  * **表格占位**：可以用Markdown表格语法输出一个表头，填入“数据见附录”或留空，或仅提示“表1 某某数据表见附件”。
  * **插入逻辑**: ReportAgent需要决定何处需要图表。简化起见，可根据内容关键字或长度判断，例如如果方法部分提到“流程”或“示意图”，可插入一个流程图占位。如果结果有数据列表，可插入表格占位。实际实现中，可在Prompt要求模型自动插入适当的图表引用。如果模型有时不会插入，开发者也可在Agent后处理阶段，根据预定义规则在特定位置插入占位符。
  * **后续填充**: 图表占位符为将来添加真实图表留出空间。可视化Agent或人工在查看报告时，根据占位符位置生成相应图表，并替换占位图片链接或补充实际图像。这种设计保证报告初稿快速产出，同时支持后续完善。

* **格式验证**：报告Agent生成Markdown后，最好进行一次格式验证。例如：

  * 是否所有需要的章节标题都存在？（没有则补齐空章节或重新提示模型）；
  * 引用和参考文献数量是否一致？
  * 图表编号是否连续、描述是否存在？
  * Markdown语法是否正确闭合（例如列表、表格格式是否正确）？
    这些可以通过简单的正则检查或渲染预览库来完成。Cursor环境下，可以利用Markdown解析库对输出进行解析，发现结构异常及时报告或修正。

**参考实现价值**：结构化、引证明确的报告输出是Deep Research系统成熟度的重要标志。例如OpenDeepResearcher系统能够生成带有层次结构和明确出处引用的综合报告。我们的报告生成Agent通过精心设计的Prompt和组装流程，实现了类似的效果——产出**层次分明、证据充分的研究报告**，极大提高了科研助理系统输出的实用性和可信度。这份报告既满足最终用户阅读需求，也为系统持续学习和审计提供了高价值的文档记录。

## 总结与模块开发建议

以上方案详细阐述了批判Agent和报告生成Agent在科研多Agent系统中的实现细节。两者分工明确：前者作为**逻辑与伦理的把关者**提升方案质量，后者作为**成果的整理者**输出结构化报告。它们通过黑板机制松耦合协作，在任务不同时刻介入，形成完整的研发闭环。开发者在Cursor中可按照本方案直接编写相应模块：

* 创建 `critique_agent.py` 实现 CritiqueAgent 类，定义订阅事件`solution_draft_created`，在 `process` 中调用 DeepSeek-R1 API 执行批判审查，输出反馈 JSON 到黑板。应包括 Prompt 模板和 JSON 字段结构化输出。
* 创建 `report_agent.py` 实现 ReportGenerationAgent 类，触发条件可由主Agent控制。在 `process` 中汇总黑板信息并调用 DeepSeek-R1 生成Markdown报告文本，最后输出到黑板并存储。实现章节划分、引用整合、图表占位等功能。
* 确保 `base_agent.py`（或调度框架）支持事件订阅和触发调用，`llm_client.py` 封装 DeepSeek-R1 接口供各Agent使用，`blackboard.py` 提供线程安全的读写和发布订阅。
* 充分利用配置/模板，使Prompt和输出格式易于调优。定期通过批判Agent的反馈改进Prompt，实现系统自我优化。
* 最终，经过以上模块分工实现，Cursor开发者能够快速组装出一个多Agent科研助手系统。它具备严谨的内部审查（批判Agent）和高质量的报告产出（报告生成Agent）能力，使整个系统更接近人类专家团队的协作方式。通过多Agent协同，系统能针对复杂科研任务给出可靠且结构完善的解决方案报告。

**参考文献**（节选）：本方案在设计时参考了当前多Agent科研系统架构和实践，包括多Agent分工与协调模式、黑板系统机制、以及报告生成与证据集成技术的最新进展等。这些参考为本方案提供了有力支撑，使设计具有前沿性和可行性。
