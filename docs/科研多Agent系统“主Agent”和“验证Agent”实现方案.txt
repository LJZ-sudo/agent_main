非常好，我将为你输出一份详细的主Agent与验证Agent的Cursor开发指导方案，基于Python语言、不依赖现成Agent框架、使用DeepSeek-R1 API，初期以串行任务调度为主、黑板机制后续自行设计。

该方案将涵盖：

* 每个模块的功能边界
* 组件结构与数据流（含黑板交互示意）
* 深度Prompt设计
* 核心逻辑伪代码/函数建议
* 面向Cursor的文件与模块组织建议

稍后我会完成内容并通知你查阅。


# 科研多Agent系统“主Agent”和“验证Agent”实现方案

本方案针对科研领域的多智能体（Multi-Agent）协作系统，详细描述“主Agent”（Main Agent）和“验证Agent”（Verification Agent）的设计与实现。设计遵循以下约束：使用 Python 语言，**不依赖**现有的 Agent 框架（如 AutoGen、LangChain 等），初期以本地类和内存结构模拟黑板机制，LLM 接口采用 DeepSeek-R1 API，并先以串行方式调度任务，后续再扩展并行能力。设计将通过清晰的模块划分和数据格式定义，确保方案具有**高度可执行性**，可直接作为 Cursor 项目中的 `.py` 文件编写依据。

## 系统概览与架构

系统采用主从式的 **调度者-工作者（Orchestrator-Worker）** 架构：由主Agent担任调度者，负责规划任务并协调专门子Agent执行。验证Agent是主Agent委派的一个专门角色，负责对过程结果进行事实核查和质量把关，相当于科研团队中的审稿人或质控员。通过多Agent分工协作，可以显著减少单个LLM产生幻觉和错误的几率；验证Agent对主Agent输出进行交叉检查，可大幅提高事实准确性，降低错误信息的风险。

系统的 **黑板** (Blackboard) 充当共享内存和通信中枢，用于在Agent之间交换信息和协调流程。黑板上记录用户提出的问题、任务计划、中间结果、Agent反馈等内容，所有Agent均通过读写黑板来实现异步协作。这种黑板机制类似一个公共公告板，Agent在其上发布和获取信息，从而实现松耦合的通信与协调。为保证各Agent协同工作的规范性，我们会**明确定义每个Agent交互的数据格式**（输入/输出结构），采用 JSON Schema 等来描述关键字段，并在读取/写入黑板时进行校验，以确保数据完整性和兼容性。

下面分别针对主Agent和验证Agent的功能定位、交互接口、Prompt设计以及模块实现进行说明，并给出两者串行协作的示例流程和项目代码结构建议。

## 主Agent（Main Agent）

### 职责与功能目标

主Agent是系统的核心**协调者和规划者**。其主要职责包括：

* **解析用户需求**：读取并理解用户提出的科研问题或创意请求（通常从黑板或接口输入获取），确定问题的目标和约束。
* **规划求解策略**：基于问题描述，制定解题的高层策略，将复杂问题**分解成一系列子任务**或步骤，并确定各步的先后顺序和所需信息。
* **任务发布与执行协调**：通过黑板发布子任务描述，或直接调用相应的子Agent执行任务。在初期实现中，由于仅有验证Agent作为协作者，主Agent可以直接执行主要的求解步骤（如查找信息、草拟答案），后续在扩展并行能力时，再将不同步骤委派给更多专门Agent并行完成。
* **整合中间结果**：持续**监控黑板**上子任务的进展和结果，收集各Agent提供的部分解答。在只有验证Agent的情形下，主Agent主要关注验证Agent反馈的意见。
* **调整策略与最终汇总**：根据黑板上反馈的结果和意见，适时调整解题策略。例如，当验证Agent指出方案存在问题时，主Agent需修改计划或答案。最终，主Agent整合所有信息，形成**最终解决方案或回答**，写回黑板并输出给用户。

主Agent 在执行过程中可采用链式思维和“慢思考”机制：即不急于直接得出答案，而是逐步推理，在黑板上记录推理步骤（如假设、依据、推导过程），供验证Agent和后续步骤参考。这种自我链式推理有助于确保对复杂问题的求解更加全面、有逻辑，并为验证Agent提供可审查的中间过程依据。

### 接口设计：输入、输出与黑板交互

**输入**：主Agent的主要输入是用户的科研问题陈述，可能包括问题描述、背景信息等。在实现上，输入可以在黑板上以特定结构存储，如：

```json
"user_query": {
  "id": "Q-20250623-001",
  "text": "请设计一种高效催化剂的合成实验方案。",
  "context": "（可选）关于该催化剂的背景信息..."
}
```

主Agent从黑板读取 `user_query` 对象，获取其中的`text`字段作为待解决的问题。

**输出**：主Agent在解决过程中会产出多个阶段性的结果，写入黑板共享给验证Agent：

* **研究计划 (plan)**：主Agent解析问题后生成的解决方案步骤计划。例如将问题分解为若干子任务。计划可以用 JSON 结构表示，包含步骤列表及说明：

```json
"plan": {
  "steps": [
    { "step_id": 1, "description": "检索文献获取已有催化剂材料及性能数据", "status": "pending" },
    { "step_id": 2, "description": "基于文献提出候选催化剂结构假设", "status": "pending" },
    { "step_id": 3, "description": "设计实验验证候选催化剂的性能", "status": "pending" }
  ],
  "goal": "高效催化剂合成方案研究计划"
}
```

* `steps` 列表中每个元素描述一个子任务，包括一个简要说明；`status`标记任务状态（初始为`pending`，完成后可更新为`done`）。在初期串行实现中，主Agent可以逐步执行这些步骤，并将完成情况标记在黑板上供验证Agent了解进展。

* **方案草稿 (draft\_answer)**：主Agent根据计划执行结果，撰写出的初步答案或方案草稿。它通常是对用户问题的直接回答或方案描述，可以包含从各步骤获得的信息综合而成的文本。为方便验证Agent审查，这一草稿也可结构化存储，例如：

```json
"draft_answer": {
  "content": "基于以上研究，我们建议的催化剂合成方案包括：1）使用A材料为前躯体...（详细方案内容）... 最终预计该催化剂可提高反应效率50%。",
  "supporting_steps": [1, 2, 3],
  "confidence": 0.75
}
```

* `content`字段是草稿文本，`supporting_steps`可以列举支撑该结论的步骤编号，`confidence`为主Agent对答案可靠性的自我评估（可选）。

上述输出均通过 `Blackboard` 接口写入共享内存。例如 `blackboard.post("plan", plan_object)`，`blackboard.post("draft_answer", draft)` 等。验证Agent会订阅或轮询黑板上相应的内容更新，获取这些输出作为自己的输入。

**黑板交互关键变量**：

* `user_query`：用户问题描述，类型为对象，包含文本和元数据。
* `plan`：研究计划，类型为对象，包含步骤列表等信息。
* `draft_answer`：方案草稿，类型为对象/字符串，包含回答内容及相关元数据。
* （后续阶段可能增加：`research_notes` 等，用于记录主Agent在黑板上的推理链，但初始实现也可直接利用 `plan.steps` 列表表示推理步骤。）

以上结构可用 JSON Schema 定义其格式要求，以确保主Agent输出的数据格式清晰且被验证Agent正确解析。

### Prompt 设计（与 DeepSeek API 交互）

主Agent完成核心智能任务需要借助 LLM（DeepSeek-R1）的推理和知识。针对主Agent的职责，我们设计两个关键 Prompt，用于实现 **任务规划** 和 **方案撰写**：

* **规划 Prompt**（生成研究计划）：引导 LLM 将用户问题分解为多步计划。该提示明确主Agent的角色是一个科研计划制定者，要求输出结构化的步骤列表。示例：

```text
你是一名经验丰富的科研规划AI，擅长将复杂科研问题拆解为可执行的步骤。  
用户提问："{用户的问题}"  

请详细分析上述问题，分解出解决该科研问题的步骤计划。  
要求：  
1. 使用中文输出。  
2. 列出连续编号的步骤，每步说明要做什么。  
3. 输出JSON格式，包含 "steps" 列表，每个步骤有 "step_id" 和 "description" 字段。  

注意：步骤应完整且逻辑合理，覆盖问题的各关键方面。
```

*设计思路*：该 Prompt 首先以身份描述让模型进入“科研规划AI”角色，然后提供用户问题，接着明确要求模型输出一个包含多步的计划，并指定了输出为 JSON 格式以便程序解析。编号和字段要求使得结果结构规范，便于主Agent提取进黑板。

* **草稿生成 Prompt**（撰写方案草稿）：在获得计划后，主Agent调用 LLM 根据计划内容撰写完整解答。Prompt 将提供生成的计划概要并要求模型利用其逐步产出最终答案。示例：

```text
你是一名科研助理AI，按照给定计划撰写问题的解答。  
用户问题：{用户的问题}  
研究计划步骤：  
1. {步骤1描述}  
2. {步骤2描述}  
3. {步骤3描述}  
...  

现在请根据上述计划执行结果，撰写该科研问题的初步解决方案草稿。  
要求：  
- 用中文完整表述方案，涵盖每个步骤的发现。  
- 语言专业、有逻辑，确保结论有依据（可提及步骤结果）。  
- 如有不确定之处也请在答案中说明。  

请在回答最后附上对结果的信心指数（0-100）%。  
```

*设计思路*：这里 Prompt 将之前生成的步骤列表插入，使 LLM了解完整的解决思路，然后要求它整合这些步骤的想定结果来撰写答案。附加让模型给出信心指数，便于主Agent记录 `confidence`（这也可在解析模型回答时提取百分数转化为0-1范围的值）。模型输出可以是自然语言的方案文本，我们再由程序封装进 `draft_answer` 结构。

在 Prompt 模板设计中，我们可以预先设置一些 few-shot 示例或者格式样例，帮助模型稳定输出所需的JSON等格式。同时，由于 DeepSeek-R1 接口兼容 OpenAI API，我们可以通过类似 OpenAI SDK 的方式调用该模型，发送上述 prompt 并获得结果。**DeepSeek-R1** 擅长复杂推理和代码等（例如支持23K token长上下文），足以应对科研方案生成任务。调用时，可使用一个专门的`deepseek_api.py`模块封装 API 请求，例如：

```python
# deepseek_api.py
def generate_completion(prompt: str, model: str = "deepseek-r1", max_tokens=2000, ...):
    # 函数内部通过HTTP请求DeepSeek服务，返回模型输出文本
    ...
```

主Agent模块将使用此接口发送上述 Prompt，并解析返回的结果。

### 模块结构与关键函数

主Agent可以实现为一个 Python 类，封装自身状态和功能。模块文件建议为 `main_agent.py`，主要内容包括：

* **类定义**：`class MainAgent:`
  属性：`blackboard`（引用共享黑板实例）、`llm_client`（LLM调用接口，例如 deepseek\_api 模块）。
  方法：

  * `plan_research(self, query: str) -> dict`：调用 LLM 完成研究计划生成任务。读取用户问题字符串，构造 **规划Prompt**，通过 `llm_client.generate_completion` 获取模型回复。然后解析模型返回的 JSON 字符串为 Python 字典 `plan`。最后，将 `plan` 写入黑板（例如 `self.blackboard.post("plan", plan)`）。
  * `draft_solution(self) -> str`：撰写方案草稿。此方法从黑板获取先前生成的`plan`（如 `plan = self.blackboard.get("plan")`），然后将用户问题和计划内容填入 **草稿生成Prompt**，调用 LLM 获取回答。解析模型返回的答案文本（以及信心指数等，如果有），封装成`draft_answer`对象写入黑板。返回草稿内容字符串用于后续处理。
  * `integrate_feedback(self) -> None`：整合验证反馈并生成最终答案。该方法在验证Agent完成审核后调用：从黑板获取`verification_report`，检查其中是否有问题列表。如果有，则基于这些反馈调整方案草稿。可以再次调用 LLM，请求根据反馈修改答案。**示例**：构造 Prompt 如“根据以下反馈修改上述方案：{列出反馈问题和建议}，要求给出修正后的方案。”，让模型生成改进版本。将改进后的方案作为`final_answer`写入黑板。如果反馈报告没有问题或仅有轻微建议，则主Agent可能直接将 draft\_answer 标记为最终答案（例如附上“已由验证Agent审核，无重大问题”之类的说明）。
  * `run(self, user_query: str) -> str`：主Agent对外的单次运行接口，用于串行地执行上述过程。实现上依次调用：`plan_research(query)`，`draft_solution()`，然后触发验证Agent运行审核（由调度器控制验证Agent的调用，见后述），待验证报告生成后，再调用`integrate_feedback()`处理反馈，最终返回`final_answer`文本。

上述函数划分清晰界定了主Agent各步骤职责，使得逻辑易于维护。初期可以简单实现 `integrate_feedback` 为读取反馈后直接调用一次 LLM修改答案；后续可根据需要扩展为更复杂的循环，如多轮修改或与验证Agent反复交互，直到方案通过验证。

## 验证Agent（Verification Agent）

### 职责与功能目标

验证Agent的定位是**审查者和质检员**，对主Agent产生的方案进行客观评估和把关。主Agent的产出（无论是中间推理还是最终答案）都需要经过验证Agent的审视，以发现其中潜在的问题或不足。其主要职责包括：

* **逻辑与事实核查**：验证Agent持续关注黑板上主Agent发布的主要中间结论和最终方案草案，对其进行审阅。具体来说，它会检查推理链是否严谨、自洽，有无前后矛盾；重要论断是否有充分依据，结论是否过度推断；方案中是否存在常识性错误或违背科学原理之处等。如果系统有信息检索步骤，它也会关注结论是否与检索到的权威资料一致（如有引用，应验证其可信度）。
* **发现问题并反馈**：当验证Agent发现问题时，它会生成详细的反馈报告。在黑板上发布**批注/建议**，指出问题所在并提出改进意见。例如：“步骤2的假设X缺乏实验数据支撑，建议补充相关文献或实验验证。”。这些反馈既可以是对事实准确性的质疑，也可以是对方案完整性的建议（比如提醒额外考虑某种边界条件）。
* **评估方案质量**（可选扩展）：验证Agent可以对方案进行打分或评级，供主Agent参考。例如针对准确性、创新性等维度给出评分。初期实现中可以不涉及复杂评分，仅提供定性的评语和建议。但保留此扩展接口有助于后续引入自动评价和排序机制。

通过履行上述职责，验证Agent相当于模拟**专家审稿**过程，对AI产生的方案进行**Peer Review**，从而提高结果的可靠性和质量。在多Agent协作中，引入这样的验证环节能够有效减少谬误传播，并提升系统整体性能。

值得注意的是，验证Agent本身也由 LLM 驱动，它并不直接访问外部数据库进行事实核对（在没有引入检索Agent的初期架构中），而是更多地依赖其训练知识和推理能力来发现问题。这类似于AI**自我反省**或Agents之间互相审视，以弥补单个模型的盲点。

### 接口设计：输入、输出与黑板交互

**输入**：验证Agent的输入主要来自黑板上主Agent的输出结果，包括：

* `plan`：研究计划（可选输入）。验证Agent可阅读主Agent制定的任务分解，如果其中某步本身就有明显问题（例如计划遗漏关键步骤），则可在反馈中指出。不过在大多数情况下，验证Agent主要关注最终方案，因此此输入不是必须。
* `draft_answer`：方案草案（主要输入）。这是验证Agent审查的核心对象。它会从黑板读取草稿内容和相关元数据。例如：

```json
draft_answer.content = "......"  (主Agent生成的方案文本)
draft_answer.supporting_steps = [1,2,3]  (草稿所基于的步骤)
draft_answer.confidence = 0.75  (主Agent对答案的置信度)
```

验证Agent将综合这些信息进行分析。它可以利用 `supporting_steps` 提供的线索，参考对应的 `plan.steps` 内容以了解每一步骤的用意，从而检查答案是否覆盖并正确利用了所有步骤的信息。

**输出**：验证Agent产出**验证报告**，写入黑板供主Agent读取。报告可以采用 JSON 结构表示，以便程序处理：

```json
"verification_report": {
  "issues": [
    { 
      "step_id": 2, 
      "issue": "假设X缺少实验数据支持。", 
      "suggestion": "请引入实验或参考数据验证该假设。" 
    },
    { 
      "step_id": null, 
      "issue": "结论关于效率提升的量化缺乏依据。", 
      "suggestion": "给出提高50%效率的依据来源或计算过程。" 
    }
  ],
  "verdict": "needs revision"
}
```

上述结构中，`issues`列表列举了发现的问题，每个问题可关联到特定步骤（如有，则注明 `step_id`，如是对整体结论的评议则该值为 `null`），并包含问题描述和改进建议。`verdict`字段给出整体结论，例如 `"needs revision"` 表示需要主Agent修改后再最终输出；如果没有发现实质性问题，也许 `verdict` 可标记为 `"approved"` 或 `"ok"`。

验证Agent将此报告通过 `blackboard.post("verification_report", report)` 写入黑板。主Agent稍后会检查该报告，决定如何处理（修改还是直接通过）。

**黑板交互关键变量**：

* `verification_report`：验证报告对象，包含问题列表和最终裁定。
* 可能还有 `review_score` 等评分字段（如果实现了评分功能）。

初期实现中，验证Agent每次仅在主Agent生成草稿后运行一次审核。其接口也可以设计为一个方法 `verify(draft_answer)` 返回上述 `verification_report` 字典。

### Prompt 设计（与 DeepSeek API 交互）

验证Agent的核心智能任务是对方案进行**批判性审查**。为此，需要设计一个高质量的 Prompt，引导 LLM 从**审稿人/审核者**的角度对输入方案进行评估，输出发现的问题和建议。参考多Agent系统的设计经验，我们强调验证Agent的 Prompt 要清晰扮演“严谨的科研审稿人”角色。示例提示：

```text
你是一位严谨的科研审稿人，对科研方案进行批判性核查。  
现在有一个研究方案的步骤计划和最终结论，请逐步审查其合理性和正确性。  

步骤计划：  
1. 假设使用材料A提高催化剂效率。  
2. 基于假设A设计实验验证催化剂性能。  
3. 根据实验结果提出优化方案。  

方案草案结论：  
"我们采用材料A作为催化剂，可将反应效率提高50%。该假设基于...（方案详细内容）..."  

请检查：  
- 每个步骤的推理是否严谨，有无知识盲区或逻辑漏洞。  
- 最终结论是否有充分依据支撑，是否存在不合理的假设。  
- 是否有遗漏的重要因素。  

如果发现问题，请指出问题所在并给出具体改进建议。  
如果方案总体合理，无重大问题，也请给出肯定结论。  

要求：以 JSON 格式输出审查结果。  
- 如有问题，输出 {"issues": [...], "verdict": "needs revision"}，其中 issues 列表每项包括 {"step_id": ?, "issue": "...", "suggestion": "..."}。  
- 如无重大问题，输出 {"issues": [], "verdict": "approved"} 并简述正面评价。"
```

上述 Prompt 首先将 LLM设定为审稿人身份，提供方案的关键内容（步骤+结论），然后明确检查要点和输出要求格式。在例子中，我们模拟了一个简单的方案片段插入 Prompt。实际实现中，程序会构造类似的提示，将黑板中的 `plan.steps` 列表和 `draft_answer.content`填入相应位置。输出采用 JSON，有利于后续解析；我们要求模型在发现问题时列举 issue 清单，以及给出总体裁定（verdict）。DeepSeek-R1 在这种分析推理任务上应该表现良好，其强大的推理能力有助于发现隐藏的问题点。

**Few-shot 提示**：为提高模型输出的准确性和格式一致性，可以在 Prompt 中加入一两个范例，让模型参照学习。例如提供一个简短的示例方案及审查输出格式。但在DeepSeek-R1模型性能足够强的情况下，此步骤可选。

验证Agent通过 `deepseek_api.generate_completion(review_prompt)` 获取模型输出文本，然后需做**结果解析**：尝试将输出解析为 JSON。如果解析失败（模型未严格遵守格式），也可以对输出做一些正则提取或让验证Agent再次梳理。不过，合理的 Prompt 设计和少量示例通常能确保模型输出符合预期格式。

### 模块结构与关键函数

验证Agent的实现也可以采用类封装，文件名建议为 `verification_agent.py`。主要内容：

* **类定义**：`class VerificationAgent:`
  属性：`blackboard`（共享黑板实例引用）、`llm_client`（LLM调用接口，同样使用 deepseek\_api）。
  方法：

  * `verify_solution(self) -> dict`：执行一次验证流程。该方法从 `blackboard` 读取主Agent产出的 `draft_answer`（必要时也读取 `plan`）。然后构造上述 **审查Prompt** 并调用 LLM 获取审核结果。解析模型输出为 Python 字典（对应 `verification_report` 结构）。最后，将该报告写回黑板（例如 `self.blackboard.post("verification_report", report)`)并返回报告。
  * （可选）`score_solution(self) -> None`：如果需要评分功能，可实现此方法从方案草稿评估各指标得分。但初期可以不实现，只保留接口。

由于验证Agent通常在主Agent完成初稿后才工作，可以在系统调度上由主Agent触发其运行。也可以设计一个独立的触发机制，例如黑板事件：当有新的 `draft_answer` 写入时，验证Agent检测到该事件，自动开始审核。为了简单起见，初期可在顺序流程中直接调用 `verify_solution()`。

VerificationAgent 类的逻辑相对单一：读取 -> 审查 -> 写入反馈。实现时要注意**健壮性**，例如当模型输出JSON格式不完整时，做好异常处理和必要的重试。另外，验证Agent在审查时也可以访问主Agent提供的置信度或中间推理，以辅助判断。

## 主-Agent 与 验证-Agent 串行协同流程示例

以下示例演示主Agent和验证Agent在一次完整任务中的串行交互过程，以及数据在黑板中的流转：

1. **任务开始**：用户通过系统前端提出问题，例如：「**设计一种高效催化剂的合成实验方案**」。该问题被封装成 `user_query` 对象写入黑板，内容如前述示例。调度器（或主程序）捕获到新任务，在黑板记录任务发布事件。
2. **主Agent规划**：主Agent读取 `user_query` 后，调用其 `plan_research()` 方法。此时构造规划Prompt并请求 DeepSeek-R1 模型。假设模型返回了3步的计划JSON（对应上文例子）。主Agent解析后，将此计划对象写入黑板的 `plan` 字段，并将每步的`status`初始化为"pending"。
   *（黑板此时数据示意）*：

   ```json
   {
     "user_query": {...},
     "plan": {
       "steps": [
         {"step_id": 1, "description": "检索文献获取已有催化剂材料及性能数据", "status": "pending"},
         {"step_id": 2, "description": "基于文献提出候选催化剂结构假设", "status": "pending"},
         {"step_id": 3, "description": "设计实验验证候选催化剂的性能", "status": "pending"}
       ],
       "goal": "高效催化剂合成方案研究计划"
     }
   }
   ```
3. **主Agent执行及草案生成**：接着，主Agent调用 `draft_solution()` 方法。它先模拟执行每个步骤（在初期实现中，可直接假定每步成功，或简单地让 LLM 基于常识产出结果），然后将步骤结果融入方案撰写Prompt中请求 LLM。DeepSeek-R1 返回了一段文字，主Agent将其视为方案草稿，例如“**我们建议采用材料A作为催化剂，可将效率提高50%...**”。主Agent将该文本封装为 `draft_answer` 写入黑板，并更新 `plan.steps` 中各步骤的状态为 "done"（假设都成功）。
   *（黑板新增数据示意）*：

   ```json
   "draft_answer": {
     "content": "我们建议采用材料A作为催化剂，可将反应效率提高50%...（下略）",
     "supporting_steps": [1,2,3],
     "confidence": 0.7
   }
   ```

   此时黑板上已有 user\_query、plan（steps已标记done）、draft\_answer 三部分内容。主Agent暂时告一段落，调度器转而通知验证Agent进行审查。
4. **验证Agent审查**：验证Agent的 `verify_solution()` 被调用后，从黑板读取 `plan` 和 `draft_answer`。它构造审查Prompt（插入步骤概要和方案草稿）并调用 DeepSeek-R1 进行分析。模型可能发现方案有两个问题：**(a)** 步骤2提出的催化剂假设缺少实验数据支持；**(b)** 最终声称效率提高50%的结论缺乏依据量化。模型按照Prompt要求输出了JSON格式的报告：包含上述两条 issues，并给出建议例如“补充实验数据验证假设”“提供效率50%的依据或计算”。验证Agent解析该输出为 `verification_report` 字典，写入黑板：

   ```json
   "verification_report": {
     "issues": [
       {"step_id": 2, "issue": "催化剂性能的假设缺少实验数据支持。", "suggestion": "建议增加实验步骤验证该假设的有效性。"},
       {"step_id": null, "issue": "效率提高50%的结论缺乏数据依据。", "suggestion": "需要说明该数值的来源，或给出计算依据。"}
     ],
     "verdict": "needs revision"
   }
   ```

   验证Agent完成审核，将报告写入黑板后结束本轮工作。
5. **主Agent整合反馈**：主Agent检测到黑板上的 `verification_report` 更新（或者通过顺序流程直接继续执行）。它调用 `integrate_feedback()` 方法读取报告内容。看到有两条 issues，主Agent据此决定需要修改方案。主Agent可采取以下措施：

   * 针对 issue1，发现自己的方案缺少验证假设X的实验，于是可以在 plan 中新增一步实验（这在当前任务后期可能不实际重开新步骤，但在下一个类似任务中可引入此经验）。眼下主Agent更可能在答案中补充说明：“（已添加考虑需要进一步实验验证该假设）”。
   * 针对 issue2，主Agent可能咨询 LLM 获取支持数据，或者降低措辞如改成“有望提高效率”而非明确50%。例如主Agent构造 Prompt：“请根据反馈，将方案草稿中‘提高50%’的表述修改为更谨慎的说法，并加入需要实验验证的数据说明。” 让 LLM 给出改写建议。DeepSeek-R1 返回修改后的方案文本。
     主Agent用获得的新文本更新 `final_answer` 字段写入黑板：

   ```json
   "final_answer": {
     "content": "我们建议采用材料A作为催化剂，预期能显著提升反应效率（具体提升幅度有待实验测定）...（其余方案内容修改完善）...",
     "approved_by": "VerificationAgent"
   }
   ```

   同时主Agent可以在 `final_answer` 中标注已通过验证Agent审核（如上`approved_by`字段）。这样最终答案在黑板上定稿。
6. **结果输出**：系统将黑板中的 `final_answer.content` 提取，作为对用户的问题解答输出给前端。整个流程结束。

在这一串行协同过程中，黑板发挥了**信息中转站**作用：主Agent和验证Agent不直接调用彼此，而是通过黑板读写来实现解耦协作。主Agent专注于求解和生成，验证Agent专注于审查和反馈，各自的输入输出格式清晰定义，保证数据交换顺畅。在并行扩展时，这种模式也方便引入更多Agent，或让验证Agent对多个并行方案分别审查后综合。

## 文件组织与模块依赖

为方便开发和维护，建议按照功能将代码模块化，组织项目结构如下：

```plaintext
research_multi_agent_system/  
├── core/  
│   ├── blackboard.py            # 黑板实现类 (共享内存数据结构及发布/订阅机制模拟)  
│   └── deepseek_api.py          # DeepSeek-R1 API 调用封装 (类似OpenAI接口的HTTP客户端)  
├── agents/  
│   ├── main_agent.py            # 主Agent模块 (MainAgent类: 规划、执行、整合反馈)  
│   ├── verification_agent.py    # 验证Agent模块 (VerificationAgent类: 审查与反馈)  
│   └── __init__.py              # 可以为空或用于初始化agent子包  
└── orchestrator.py              # 调度脚本/模块：初始化黑板和Agent，顺序执行Main->Verification->整合->输出结果  
```

**模块依赖关系**：

* `main_agent.py` 和 `verification_agent.py` 都会导入 `core.blackboard` 模块，以访问黑板类。它们还依赖 `core.deepseek_api` 用于LLM调用。两者之间不直接相互调用，而是通过黑板间接通信。
* `blackboard.py` 实现一个全局共享的 `Blackboard` 类，内部使用线程安全的方式存储数据（例如用字典加锁机制）。提供方法如 `post(key, value)`，`get(key)`，以及将来扩展的 `subscribe(event_type, agent)` 等。初期可简化为所有Agent主动轮询黑板，无需复杂订阅。
* `orchestrator.py` 是主入口，负责创建一个 Blackboard 实例和 DeepSeek API 客户端实例，初始化主Agent和验证Agent对象，并按顺序调用主Agent -> 验证Agent -> 主Agent反馈整合的流程（如上一节示例）。调度流程可根据需要写成脚本形式或封装成 Orchestrator 类。初期采用串行执行，日后可改为并发（例如使用 `asyncio` 或多线程）来并行触发任务。

**并发扩展考虑**：未来为了提高效率，可在调度器中让主Agent分解的多个子任务并行运行不同Agent。例如通过启动多个线程或 `asyncio` 协程，让信息检索Agent与建模Agent等同时工作，然后汇总到黑板。不过需要确保 `Blackboard` 的并发安全，例如在 `post` 方法中加锁或使用异步队列机制。此外，可引入事件驱动模型（publish/subscribe）：当黑板有新数据时自动通知相应Agent处理。初期版本中，我们已经将各Agent逻辑解耦至独立模块，因而在实现并行时，只需替换调度策略（例如利用 `asyncio.gather` 并发调用不同Agent方法）而不需大改各Agent代码。

综上，本方案详细定义了主Agent与验证Agent的职责边界、交互数据结构和关键算法步骤，提供了清晰的 Prompt 模板和模块划分设计。开发者可据此直接编写相应 `.py` 文件，实现一个可运行的科研多Agent协同原型系统。在实际编码时，请遵循以上接口约定和数据结构格式，以确保主Agent和验证Agent之间通过黑板顺利通信协作，逐步构建起一个可靠的科研助理多智能体系统。通过不断迭代完善Prompt和增加Agent类型，系统将能处理更复杂的科研创意任务并产出高质量的解决方案。
